

============================== 2023-07-18 05:25:18.053128 | 619331d2-ccbb-4bc5-b3aa-f569473b1bbe ==============================
[0m05:25:18.053170 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:25:18.053737 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:25:18.053886 [debug] [MainThread]: Tracking: tracking
[0m05:25:18.101636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0312b3bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0312b3d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0312b3cd0>]}
[0m05:25:18.114710 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m05:25:18.114981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0312b3bb0>]}
[0m05:25:18.150007 [debug] [MainThread]: Parsing macros/statement.sql
[0m05:25:18.152245 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m05:25:18.158016 [debug] [MainThread]: Parsing macros/catalog.sql
[0m05:25:18.159703 [debug] [MainThread]: Parsing macros/adapters.sql
[0m05:25:18.176246 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m05:25:18.182526 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m05:25:18.182954 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m05:25:18.185880 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m05:25:18.201159 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m05:25:18.202317 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m05:25:18.208316 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:25:18.209086 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:25:18.209936 [debug] [MainThread]: Parsing macros/adapters.sql
[0m05:25:18.242391 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m05:25:18.244644 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m05:25:18.251478 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m05:25:18.251879 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m05:25:18.256480 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m05:25:18.276539 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m05:25:18.278060 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m05:25:18.281643 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m05:25:18.287864 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m05:25:18.293014 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m05:25:18.293391 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m05:25:18.294268 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:25:18.298406 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m05:25:18.298679 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m05:25:18.299885 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:25:18.310301 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m05:25:18.310789 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m05:25:18.311203 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m05:25:18.311569 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m05:25:18.312544 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m05:25:18.312957 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m05:25:18.313397 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m05:25:18.316075 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m05:25:18.317650 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m05:25:18.318869 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m05:25:18.330635 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m05:25:18.340431 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m05:25:18.349337 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m05:25:18.352434 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m05:25:18.353651 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m05:25:18.354871 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m05:25:18.360448 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m05:25:18.371992 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m05:25:18.373041 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m05:25:18.377672 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m05:25:18.385005 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m05:25:18.397545 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m05:25:18.401426 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m05:25:18.403798 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m05:25:18.407622 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m05:25:18.408494 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m05:25:18.410818 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m05:25:18.412360 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m05:25:18.417376 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m05:25:18.430803 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m05:25:18.431812 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m05:25:18.433483 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m05:25:18.434535 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m05:25:18.435143 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m05:25:18.435696 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m05:25:18.436163 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m05:25:18.437094 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m05:25:18.440747 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m05:25:18.446636 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m05:25:18.447198 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m05:25:18.448029 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m05:25:18.448690 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m05:25:18.449339 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:25:18.450196 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m05:25:18.450757 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m05:25:18.451473 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m05:25:18.452313 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m05:25:18.453930 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:25:18.454800 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m05:25:18.455535 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m05:25:18.456272 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m05:25:18.456981 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m05:25:18.457608 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m05:25:18.458349 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m05:25:18.458983 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m05:25:18.463849 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m05:25:18.464579 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m05:25:18.465218 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m05:25:18.466450 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m05:25:18.467993 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m05:25:18.468695 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m05:25:18.469708 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m05:25:18.470419 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m05:25:18.471843 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m05:25:18.474077 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m05:25:18.475966 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m05:25:18.486372 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m05:25:18.487729 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m05:25:18.497089 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m05:25:18.500114 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m05:25:18.505090 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m05:25:18.511904 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m05:25:18.516191 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m05:25:18.772847 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_gold_churn_features.sql
[0m05:25:18.782091 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_events.sql
[0m05:25:18.784323 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_users.sql
[0m05:25:18.786552 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_orders.sql
[0m05:25:18.881996 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging

[0m05:25:18.886450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048ee20d0>]}
[0m05:25:18.893608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048e475e0>]}
[0m05:25:18.893904 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:25:18.894142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048e97a90>]}
[0m05:25:18.895540 [info ] [MainThread]: 
[0m05:25:18.896195 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:25:18.897158 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m05:25:18.897446 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m05:25:18.897575 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m05:25:18.897683 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:25:20.112054 [debug] [ThreadPool]: SQL status: OK in 1.21 seconds
[0m05:25:20.210327 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m05:25:20.365430 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_dbdemos"
[0m05:25:20.373454 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:20.373616 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_dbdemos"
[0m05:25:20.373740 [debug] [ThreadPool]: On list_hive_metastore_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_hive_metastore_dbdemos"} */

      select current_catalog()
  
[0m05:25:20.373849 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:25:20.782635 [debug] [ThreadPool]: SQL status: OK in 0.41 seconds
[0m05:25:20.789739 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_dbdemos"
[0m05:25:20.789920 [debug] [ThreadPool]: On list_hive_metastore_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_hive_metastore_dbdemos"} */
show table extended in hive_metastore.dbdemos like '*'
  
[0m05:25:22.696241 [debug] [ThreadPool]: SQL status: OK in 1.91 seconds
[0m05:25:22.700509 [debug] [ThreadPool]: On list_hive_metastore_dbdemos: ROLLBACK
[0m05:25:22.700742 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:25:22.700862 [debug] [ThreadPool]: On list_hive_metastore_dbdemos: Close
[0m05:25:22.848268 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_dbdemos_dbt_test__audit"
[0m05:25:22.850567 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:22.850727 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_dbdemos_dbt_test__audit"
[0m05:25:22.850842 [debug] [ThreadPool]: On list_hive_metastore_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_hive_metastore_dbdemos_dbt_test__audit"} */

      select current_catalog()
  
[0m05:25:22.850942 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:25:23.232442 [debug] [ThreadPool]: SQL status: OK in 0.38 seconds
[0m05:25:23.236618 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_dbdemos_dbt_test__audit"
[0m05:25:23.236787 [debug] [ThreadPool]: On list_hive_metastore_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_hive_metastore_dbdemos_dbt_test__audit"} */
show table extended in hive_metastore.dbdemos_dbt_test__audit like '*'
  
[0m05:25:23.709541 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
[0m05:25:23.712200 [debug] [ThreadPool]: On list_hive_metastore_dbdemos_dbt_test__audit: ROLLBACK
[0m05:25:23.712409 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:25:23.712518 [debug] [ThreadPool]: On list_hive_metastore_dbdemos_dbt_test__audit: Close
[0m05:25:23.885496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048fd4940>]}
[0m05:25:23.885892 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:23.886013 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:25:23.886375 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m05:25:23.886611 [info ] [MainThread]: 
[0m05:25:23.892279 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:25:23.892616 [info ] [Thread-1  ]: 1 of 4 START sql table model dbdemos.dbt_c360_silver_events .................... [RUN]
[0m05:25:23.893201 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:25:23.893343 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:25:23.893467 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:25:23.895907 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:25:23.896843 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:23.897031 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:25:23.941063 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:25:23.942222 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:23.942434 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:25:23.942564 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

  
    
        create or replace table hive_metastore.dbdemos.dbt_c360_silver_events
      
      
    using delta
      
      
      
      
      
      
      as
      

select 
  user_id,
  session_id,
  event_id,
  `date`,
  platform,
  action,
  url
from dbdemos.dbt_c360_bronze_events
  
[0m05:25:23.942662 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:25:28.114675 [debug] [Thread-1  ]: SQL status: OK in 4.17 seconds
[0m05:25:28.131450 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:28.131643 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: ROLLBACK
[0m05:25:28.131764 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:25:28.131871 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: Close
[0m05:25:28.282348 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd030ff5340>]}
[0m05:25:28.282705 [info ] [Thread-1  ]: 1 of 4 OK created sql table model dbdemos.dbt_c360_silver_events ............... [[32mOK[0m in 4.39s]
[0m05:25:28.283245 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:25:28.283405 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:25:28.283662 [info ] [Thread-1  ]: 2 of 4 START sql table model dbdemos.dbt_c360_silver_orders .................... [RUN]
[0m05:25:28.284113 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:25:28.284241 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:25:28.284353 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:25:28.286827 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:25:28.287463 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:28.287601 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:25:28.292066 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:25:28.292695 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:28.292843 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:25:28.292986 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

  
    
        create or replace table hive_metastore.dbdemos.dbt_c360_silver_orders
      
      
    using delta
      
      
      
      
      
      
      as
      

--notes: order data cleaned and anonymized for analysis -- 
select
  cast(amount as int),
  `id` as order_id,
  user_id,
  cast(item_count as int),
  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date
from dbdemos.dbt_c360_bronze_orders
  
[0m05:25:28.293100 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:25:32.263690 [debug] [Thread-1  ]: SQL status: OK in 3.97 seconds
[0m05:25:32.266608 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:32.266802 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: ROLLBACK
[0m05:25:32.266920 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:25:32.267031 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: Close
[0m05:25:32.415858 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048d7aa30>]}
[0m05:25:32.416256 [info ] [Thread-1  ]: 2 of 4 OK created sql table model dbdemos.dbt_c360_silver_orders ............... [[32mOK[0m in 4.13s]
[0m05:25:32.416590 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:25:32.416743 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:25:32.416916 [info ] [Thread-1  ]: 3 of 4 START sql table model dbdemos.dbt_c360_silver_users ..................... [RUN]
[0m05:25:32.417614 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:25:32.417915 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:25:32.418071 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:25:32.420615 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:25:32.421233 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:32.421370 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:25:32.425956 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:25:32.426663 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:32.426863 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:25:32.427012 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

  
    
        create or replace table hive_metastore.dbdemos.dbt_c360_silver_users
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: user data cleaned and anonymized for analysis -- 
select
  id as user_id,
  sha1(email) as email, 
  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, 
  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, 
  initcap(firstname) as firstname, 
  initcap(lastname) as lastname, 
  address, 
  canal, 
  country,
  cast(gender as int),
  cast(age_group as int), 
  cast(churn as int) as churn
from dbdemos.dbt_c360_bronze_users
  
[0m05:25:32.427122 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:25:36.682699 [debug] [Thread-1  ]: SQL status: OK in 4.26 seconds
[0m05:25:36.685910 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:36.686130 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: ROLLBACK
[0m05:25:36.686251 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:25:36.686351 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: Close
[0m05:25:36.842438 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0290f5df0>]}
[0m05:25:36.842856 [info ] [Thread-1  ]: 3 of 4 OK created sql table model dbdemos.dbt_c360_silver_users ................ [[32mOK[0m in 4.43s]
[0m05:25:36.843420 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:25:36.844109 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:25:36.844343 [info ] [Thread-1  ]: 4 of 4 START sql table model dbdemos.dbt_c360_gold_churn_features .............. [RUN]
[0m05:25:36.844882 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:25:36.845009 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:25:36.845122 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:25:36.849149 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:25:36.850026 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:36.850282 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:25:36.858138 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:25:36.859034 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:36.859328 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:25:36.859513 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

  
    
        create or replace table hive_metastore.dbdemos.dbt_c360_gold_churn_features
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: final user table with all information for Analysis / ML -- 
with 
    -- block 1 -- 
    churn_orders_stats as 
    (select user_id, 
            count(*) as order_count, 
            sum(amount) as total_amount, 
            sum(item_count) as total_item, 
             max(creation_date) as last_transaction
      from hive_metastore.dbdemos.dbt_c360_silver_orders 
      group by user_id
    ),  
    -- block 2 -- 
    churn_app_events_stats as 
    (
      select first(platform) as platform, 
             user_id, 
             count(*) as event_count, 
             count(distinct session_id) as session_count, 
             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event
       from hive_metastore.dbdemos.dbt_c360_silver_events 
       group by user_id
    )

select *, 
       datediff(now(), creation_date) as days_since_creation,
       datediff(now(), last_activity_date) as days_since_last_activity,
       datediff(now(), last_event) as days_last_event
from hive_metastore.dbdemos.dbt_c360_silver_users 
inner join churn_orders_stats using (user_id)
inner join churn_app_events_stats using (user_id)
  
[0m05:25:36.859691 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:25:41.786537 [debug] [Thread-1  ]: SQL status: OK in 4.93 seconds
[0m05:25:41.788794 [debug] [Thread-1  ]: finished collecting timing info
[0m05:25:41.788958 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: ROLLBACK
[0m05:25:41.789071 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:25:41.789179 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: Close
[0m05:25:41.938494 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '619331d2-ccbb-4bc5-b3aa-f569473b1bbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048f531c0>]}
[0m05:25:41.938868 [info ] [Thread-1  ]: 4 of 4 OK created sql table model dbdemos.dbt_c360_gold_churn_features ......... [[32mOK[0m in 5.09s]
[0m05:25:41.939178 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:25:41.940232 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:25:41.940461 [debug] [MainThread]: On master: ROLLBACK
[0m05:25:41.940567 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:25:42.102902 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:25:42.103206 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:25:42.103317 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:25:42.103432 [debug] [MainThread]: On master: ROLLBACK
[0m05:25:42.103531 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:25:42.103631 [debug] [MainThread]: On master: Close
[0m05:25:42.258121 [info ] [MainThread]: 
[0m05:25:42.258446 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 23.36 seconds (23.36s).
[0m05:25:42.258644 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:25:42.258759 [debug] [MainThread]: Connection 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features' was properly closed.
[0m05:25:42.267714 [info ] [MainThread]: 
[0m05:25:42.268005 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:25:42.268221 [info ] [MainThread]: 
[0m05:25:42.268402 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m05:25:42.268728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048f51eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048f51ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd048f53670>]}
[0m05:25:42.268970 [debug] [MainThread]: Flushing usage events


============================== 2023-07-18 05:26:43.963256 | 0978e63d-402f-4d9d-86e5-1493c7f3bcdd ==============================
[0m05:26:43.963306 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:26:43.964038 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:26:43.964230 [debug] [MainThread]: Tracking: tracking
[0m05:26:44.002512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a3dbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a3dd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a3dcd0>]}
[0m05:26:44.034812 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m05:26:44.035140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb888bf1250>]}
[0m05:26:44.072165 [debug] [MainThread]: Parsing macros/statement.sql
[0m05:26:44.074228 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m05:26:44.079865 [debug] [MainThread]: Parsing macros/catalog.sql
[0m05:26:44.081485 [debug] [MainThread]: Parsing macros/adapters.sql
[0m05:26:44.097922 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m05:26:44.104247 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m05:26:44.104666 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m05:26:44.107592 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m05:26:44.123040 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m05:26:44.124172 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m05:26:44.130029 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:26:44.130782 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:26:44.131621 [debug] [MainThread]: Parsing macros/adapters.sql
[0m05:26:44.164223 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m05:26:44.166571 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m05:26:44.173474 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m05:26:44.173879 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m05:26:44.178511 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m05:26:44.198531 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m05:26:44.200044 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m05:26:44.203633 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m05:26:44.209901 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m05:26:44.215087 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m05:26:44.215496 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m05:26:44.216589 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:26:44.221034 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m05:26:44.221443 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m05:26:44.222724 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:26:44.233134 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m05:26:44.233584 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m05:26:44.233962 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m05:26:44.234322 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m05:26:44.235271 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m05:26:44.235665 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m05:26:44.236095 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m05:26:44.239212 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m05:26:44.240817 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m05:26:44.241972 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m05:26:44.253601 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m05:26:44.263863 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m05:26:44.273534 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m05:26:44.276722 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m05:26:44.277934 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m05:26:44.279129 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m05:26:44.285153 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m05:26:44.297566 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m05:26:44.298769 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m05:26:44.303542 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m05:26:44.311759 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m05:26:44.324906 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m05:26:44.328981 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m05:26:44.331844 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m05:26:44.335930 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m05:26:44.336887 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m05:26:44.339269 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m05:26:44.340925 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m05:26:44.346466 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m05:26:44.360177 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m05:26:44.361428 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m05:26:44.363195 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m05:26:44.364263 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m05:26:44.364871 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m05:26:44.365404 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m05:26:44.365851 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m05:26:44.366738 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m05:26:44.370182 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m05:26:44.376619 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m05:26:44.377344 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m05:26:44.378228 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m05:26:44.378920 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m05:26:44.379592 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m05:26:44.380466 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m05:26:44.381049 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m05:26:44.381772 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m05:26:44.382647 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m05:26:44.384305 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m05:26:44.385172 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m05:26:44.385920 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m05:26:44.386651 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m05:26:44.387367 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m05:26:44.388001 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m05:26:44.388738 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m05:26:44.389361 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m05:26:44.394288 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m05:26:44.395012 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m05:26:44.395643 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m05:26:44.396886 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m05:26:44.398442 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m05:26:44.399153 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m05:26:44.400173 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m05:26:44.400901 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m05:26:44.402343 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m05:26:44.404589 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m05:26:44.406498 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m05:26:44.416940 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m05:26:44.418294 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m05:26:44.427761 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m05:26:44.430807 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m05:26:44.435812 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m05:26:44.442674 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m05:26:44.446964 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m05:26:44.703006 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_gold_churn_features.sql
[0m05:26:44.712493 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_events.sql
[0m05:26:44.714713 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_users.sql
[0m05:26:44.717032 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_orders.sql
[0m05:26:44.812081 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging

[0m05:26:44.816778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb859396a90>]}
[0m05:26:44.824562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb84807faf0>]}
[0m05:26:44.824876 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:26:44.825097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8592be160>]}
[0m05:26:44.826477 [info ] [MainThread]: 
[0m05:26:44.827414 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:26:44.828413 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev"
[0m05:26:44.828639 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m05:26:44.828768 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=asong_dev, schema=None)
[0m05:26:44.828878 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:26:45.738941 [debug] [ThreadPool]: SQL status: OK in 0.91 seconds
[0m05:26:45.742875 [debug] [ThreadPool]: On list_asong_dev: Close
[0m05:26:45.894079 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev_dbdemos"
[0m05:26:45.902116 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:45.902265 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m05:26:45.902391 [debug] [ThreadPool]: On list_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos"} */

      select current_catalog()
  
[0m05:26:45.902495 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:26:46.283308 [debug] [ThreadPool]: SQL status: OK in 0.38 seconds
[0m05:26:46.291286 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m05:26:46.291476 [debug] [ThreadPool]: On list_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos"} */
show table extended in asong_dev.dbdemos like '*'
  
[0m05:26:47.157607 [debug] [ThreadPool]: SQL status: OK in 0.87 seconds
[0m05:26:47.160687 [debug] [ThreadPool]: On list_asong_dev_dbdemos: ROLLBACK
[0m05:26:47.160898 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:26:47.160999 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m05:26:47.313323 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m05:26:47.315461 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:47.315616 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m05:26:47.315741 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */

      select current_catalog()
  
[0m05:26:47.315852 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:26:47.706995 [debug] [ThreadPool]: SQL status: OK in 0.39 seconds
[0m05:26:47.710260 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m05:26:47.710413 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */
show table extended in asong_dev.dbdemos_dbt_test__audit like '*'
  
[0m05:26:48.039510 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */
show table extended in asong_dev.dbdemos_dbt_test__audit like '*'
  
[0m05:26:48.039761 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m05:26:48.039878 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [SCHEMA_NOT_FOUND] org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:676)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:554)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:405)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:383)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:368)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:417)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.requireScExists(ManagedCatalogSessionCatalog.scala:259)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1521)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1602)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:299)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$48(tables.scala:1139)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:1139)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:267)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:267)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:230)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:429)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:175)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:379)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:244)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:262)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:250)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:250)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:250)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:204)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:195)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:254)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:497)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:590)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:696)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:590)
	... 21 more

[0m05:26:48.039998 [debug] [ThreadPool]: Databricks adapter: operation-id: b'\x01\xee%+\xb2?\x15t\xa8\xaf7\xc63("Q'
[0m05:26:48.040405 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m05:26:48.040514 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m05:26:48.040718 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about asong_dev.dbdemos_dbt_test__audit: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m05:26:48.040850 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: ROLLBACK
[0m05:26:48.040952 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:26:48.041050 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m05:26:48.200411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb859396d90>]}
[0m05:26:48.200750 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:48.200873 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:26:48.201252 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m05:26:48.201470 [info ] [MainThread]: 
[0m05:26:48.205914 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:26:48.206221 [info ] [Thread-1  ]: 1 of 4 START sql table model dbdemos.dbt_c360_silver_events .................... [RUN]
[0m05:26:48.206769 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:26:48.206915 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:26:48.207048 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:26:48.209729 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:26:48.210318 [debug] [Thread-1  ]: finished collecting timing info
[0m05:26:48.210464 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:26:48.251096 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:26:48.251731 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:48.251847 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m05:26:48.251964 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_events
      
      
    using delta
      
      
      
      
      
      
      as
      

select 
  user_id,
  session_id,
  event_id,
  `date`,
  platform,
  action,
  url
from dbdemos.dbt_c360_bronze_events
  
[0m05:26:48.252068 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:26:52.604991 [debug] [Thread-1  ]: SQL status: OK in 4.35 seconds
[0m05:26:52.620073 [debug] [Thread-1  ]: finished collecting timing info
[0m05:26:52.620252 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: ROLLBACK
[0m05:26:52.620370 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:26:52.620478 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: Close
[0m05:26:52.775136 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a82fd0>]}
[0m05:26:52.775517 [info ] [Thread-1  ]: 1 of 4 OK created sql table model dbdemos.dbt_c360_silver_events ............... [[32mOK[0m in 4.57s]
[0m05:26:52.776050 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m05:26:52.776203 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:26:52.776379 [info ] [Thread-1  ]: 2 of 4 START sql table model dbdemos.dbt_c360_silver_orders .................... [RUN]
[0m05:26:52.776965 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:26:52.777122 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:26:52.777246 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:26:52.781291 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:26:52.781934 [debug] [Thread-1  ]: finished collecting timing info
[0m05:26:52.782074 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:26:52.832549 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:26:52.833313 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:52.833441 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m05:26:52.833581 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_orders
      
      
    using delta
      
      
      
      
      
      
      as
      

--notes: order data cleaned and anonymized for analysis -- 
select
  cast(amount as int),
  `id` as order_id,
  user_id,
  cast(item_count as int),
  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date
from dbdemos.dbt_c360_bronze_orders
  
[0m05:26:52.833696 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:26:56.948895 [debug] [Thread-1  ]: SQL status: OK in 4.12 seconds
[0m05:26:56.951074 [debug] [Thread-1  ]: finished collecting timing info
[0m05:26:56.951256 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: ROLLBACK
[0m05:26:56.951372 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:26:56.951477 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: Close
[0m05:26:57.106705 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a1c0d0>]}
[0m05:26:57.107112 [info ] [Thread-1  ]: 2 of 4 OK created sql table model dbdemos.dbt_c360_silver_orders ............... [[32mOK[0m in 4.33s]
[0m05:26:57.107435 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m05:26:57.107597 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:26:57.107844 [info ] [Thread-1  ]: 3 of 4 START sql table model dbdemos.dbt_c360_silver_users ..................... [RUN]
[0m05:26:57.108372 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:26:57.108509 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:26:57.108626 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:26:57.112199 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:26:57.112862 [debug] [Thread-1  ]: finished collecting timing info
[0m05:26:57.113009 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:26:57.117330 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:26:57.118168 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:26:57.118316 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m05:26:57.118457 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_users
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: user data cleaned and anonymized for analysis -- 
select
  id as user_id,
  sha1(email) as email, 
  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, 
  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, 
  initcap(firstname) as firstname, 
  initcap(lastname) as lastname, 
  address, 
  canal, 
  country,
  cast(gender as int),
  cast(age_group as int), 
  cast(churn as int) as churn
from dbdemos.dbt_c360_bronze_users
  
[0m05:26:57.118561 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:27:01.577633 [debug] [Thread-1  ]: SQL status: OK in 4.46 seconds
[0m05:27:01.579657 [debug] [Thread-1  ]: finished collecting timing info
[0m05:27:01.579838 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: ROLLBACK
[0m05:27:01.579971 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:27:01.580074 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: Close
[0m05:27:01.736994 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb878a82be0>]}
[0m05:27:01.737345 [info ] [Thread-1  ]: 3 of 4 OK created sql table model dbdemos.dbt_c360_silver_users ................ [[32mOK[0m in 4.63s]
[0m05:27:01.737674 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m05:27:01.738323 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:27:01.738689 [info ] [Thread-1  ]: 4 of 4 START sql table model dbdemos.dbt_c360_gold_churn_features .............. [RUN]
[0m05:27:01.739259 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:27:01.739394 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:27:01.739511 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:27:01.743595 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:27:01.744303 [debug] [Thread-1  ]: finished collecting timing info
[0m05:27:01.744498 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:27:01.749056 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:27:01.749673 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m05:27:01.749806 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m05:27:01.749965 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_gold_churn_features
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: final user table with all information for Analysis / ML -- 
with 
    -- block 1 -- 
    churn_orders_stats as 
    (select user_id, 
            count(*) as order_count, 
            sum(amount) as total_amount, 
            sum(item_count) as total_item, 
             max(creation_date) as last_transaction
      from asong_dev.dbdemos.dbt_c360_silver_orders 
      group by user_id
    ),  
    -- block 2 -- 
    churn_app_events_stats as 
    (
      select first(platform) as platform, 
             user_id, 
             count(*) as event_count, 
             count(distinct session_id) as session_count, 
             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event
       from asong_dev.dbdemos.dbt_c360_silver_events 
       group by user_id
    )

select *, 
       datediff(now(), creation_date) as days_since_creation,
       datediff(now(), last_activity_date) as days_since_last_activity,
       datediff(now(), last_event) as days_last_event
from asong_dev.dbdemos.dbt_c360_silver_users 
inner join churn_orders_stats using (user_id)
inner join churn_app_events_stats using (user_id)
  
[0m05:27:01.750074 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m05:27:06.380565 [debug] [Thread-1  ]: SQL status: OK in 4.63 seconds
[0m05:27:06.383009 [debug] [Thread-1  ]: finished collecting timing info
[0m05:27:06.383207 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: ROLLBACK
[0m05:27:06.383331 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m05:27:06.383444 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: Close
[0m05:27:06.558416 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0978e63d-402f-4d9d-86e5-1493c7f3bcdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8593a3f70>]}
[0m05:27:06.558857 [info ] [Thread-1  ]: 4 of 4 OK created sql table model dbdemos.dbt_c360_gold_churn_features ......... [[32mOK[0m in 4.82s]
[0m05:27:06.559211 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m05:27:06.560409 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:27:06.560590 [debug] [MainThread]: On master: ROLLBACK
[0m05:27:06.560703 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:27:06.730517 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:27:06.730807 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:27:06.730928 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:27:06.731048 [debug] [MainThread]: On master: ROLLBACK
[0m05:27:06.731149 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:27:06.731246 [debug] [MainThread]: On master: Close
[0m05:27:06.883626 [info ] [MainThread]: 
[0m05:27:06.883910 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 22.06 seconds (22.06s).
[0m05:27:06.884110 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:27:06.884219 [debug] [MainThread]: Connection 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features' was properly closed.
[0m05:27:06.892965 [info ] [MainThread]: 
[0m05:27:06.893274 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:27:06.893688 [info ] [MainThread]: 
[0m05:27:06.893909 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m05:27:06.894187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb85924a9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb85924a970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb85924ad30>]}
[0m05:27:06.894387 [debug] [MainThread]: Flushing usage events


============================== 2023-07-19 00:43:11.925003 | 9da3de96-46ac-4331-b5b4-62934fdc94c3 ==============================
[0m00:43:11.925068 [info ] [MainThread]: Running with dbt=1.3.1
[0m00:43:11.925925 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360/dbt_project', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:43:11.926080 [debug] [MainThread]: Tracking: tracking
[0m00:43:12.007556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae10fb6be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae10fb6d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae10fb6d00>]}
[0m00:43:12.080998 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:43:12.081216 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:43:12.081692 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging

[0m00:43:12.086889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fade16ce0d0>]}
[0m00:43:12.095059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae11131790>]}
[0m00:43:12.095340 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:43:12.095574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae11131820>]}
[0m00:43:12.097040 [info ] [MainThread]: 
[0m00:43:12.097673 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m00:43:12.098715 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev"
[0m00:43:12.098897 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m00:43:12.099089 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=asong_dev, schema=None)
[0m00:43:12.099202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:43:31.499122 [debug] [ThreadPool]: SQL status: OK in 19.4 seconds
[0m00:43:31.514130 [debug] [ThreadPool]: On list_asong_dev: Close
[0m00:43:33.071178 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m00:43:33.079565 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:33.079725 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m00:43:33.079854 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */

      select current_catalog()
  
[0m00:43:33.079964 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:43:37.192124 [debug] [ThreadPool]: SQL status: OK in 4.11 seconds
[0m00:43:37.198364 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m00:43:37.198535 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */
show table extended in asong_dev.dbdemos_dbt_test__audit like '*'
  
[0m00:43:38.668626 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos_dbt_test__audit"} */
show table extended in asong_dev.dbdemos_dbt_test__audit like '*'
  
[0m00:43:38.668912 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m00:43:38.669073 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [SCHEMA_NOT_FOUND] org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:676)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:554)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:405)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:383)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:368)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:417)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.requireScExists(ManagedCatalogSessionCatalog.scala:259)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1521)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1602)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:299)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$48(tables.scala:1139)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:1139)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:267)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:267)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:230)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:429)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:175)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:379)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:244)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:262)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:250)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:250)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:250)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:204)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:195)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:254)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:497)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:590)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:696)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:590)
	... 21 more

[0m00:43:38.669195 [debug] [ThreadPool]: Databricks adapter: operation-id: b'\x01\xee%\xcdL~\x14$\x85\x06Q\xf4N\x10\xa7\x04'
[0m00:43:38.669598 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m00:43:38.669714 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m00:43:38.669933 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about asong_dev.dbdemos_dbt_test__audit: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `dbdemos_dbt_test__audit` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m00:43:38.670068 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: ROLLBACK
[0m00:43:38.670174 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:43:38.670275 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m00:43:39.064030 [debug] [ThreadPool]: Acquiring new databricks connection "list_asong_dev_dbdemos"
[0m00:43:39.066074 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:39.066219 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m00:43:39.066344 [debug] [ThreadPool]: On list_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos"} */

      select current_catalog()
  
[0m00:43:39.066449 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:43:39.966804 [debug] [ThreadPool]: SQL status: OK in 0.9 seconds
[0m00:43:39.970977 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m00:43:39.971198 [debug] [ThreadPool]: On list_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "list_asong_dev_dbdemos"} */
show table extended in asong_dev.dbdemos like '*'
  
[0m00:43:42.732846 [debug] [ThreadPool]: SQL status: OK in 2.76 seconds
[0m00:43:42.737012 [debug] [ThreadPool]: On list_asong_dev_dbdemos: ROLLBACK
[0m00:43:42.737188 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:43:42.737306 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m00:43:42.971481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae11131490>]}
[0m00:43:42.971842 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:42.971965 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:43:42.972345 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m00:43:42.972560 [info ] [MainThread]: 
[0m00:43:42.979034 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:43:42.979361 [info ] [Thread-1  ]: 1 of 4 START sql table model dbdemos.dbt_c360_silver_events .................... [RUN]
[0m00:43:42.979926 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:43:42.980081 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:43:42.980225 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:43:42.985112 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:43:42.986171 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:42.986364 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:43:43.029745 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:43:43.030436 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:43.030558 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:43:43.030685 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_events
      
      
    using delta
      
      
      
      
      
      
      as
      

select 
  user_id,
  session_id,
  event_id,
  `date`,
  platform,
  action,
  url
from dbdemos.dbt_c360_bronze_events
  
[0m00:43:43.030791 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:43:47.854843 [debug] [Thread-1  ]: SQL status: OK in 4.82 seconds
[0m00:43:47.873070 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:47.873314 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: ROLLBACK
[0m00:43:47.873432 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:43:47.873543 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: Close
[0m00:43:48.498703 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae006ba0d0>]}
[0m00:43:48.499284 [info ] [Thread-1  ]: 1 of 4 OK created sql table model dbdemos.dbt_c360_silver_events ............... [[32mOK[0m in 5.52s]
[0m00:43:48.500044 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:43:48.500415 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:43:48.500718 [info ] [Thread-1  ]: 2 of 4 START sql table model dbdemos.dbt_c360_silver_orders .................... [RUN]
[0m00:43:48.501247 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:43:48.501383 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:43:48.501506 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:43:48.504151 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:43:48.506077 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:48.506257 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:43:48.510555 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:43:48.511353 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:48.511508 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:43:48.511653 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_orders
      
      
    using delta
      
      
      
      
      
      
      as
      

--notes: order data cleaned and anonymized for analysis -- 
select
  cast(amount as int),
  `id` as order_id,
  user_id,
  cast(item_count as int),
  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date
from dbdemos.dbt_c360_bronze_orders
  
[0m00:43:48.511760 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:43:52.565799 [debug] [Thread-1  ]: SQL status: OK in 4.05 seconds
[0m00:43:52.568867 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:52.569037 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: ROLLBACK
[0m00:43:52.569155 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:43:52.569262 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: Close
[0m00:43:52.836939 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae00643550>]}
[0m00:43:52.837327 [info ] [Thread-1  ]: 2 of 4 OK created sql table model dbdemos.dbt_c360_silver_orders ............... [[32mOK[0m in 4.34s]
[0m00:43:52.837670 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:43:52.837829 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:43:52.838250 [info ] [Thread-1  ]: 3 of 4 START sql table model dbdemos.dbt_c360_silver_users ..................... [RUN]
[0m00:43:52.838896 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:43:52.839136 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:43:52.839276 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:43:52.841899 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:43:52.843756 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:52.843996 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:43:52.851586 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:43:52.852471 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:52.852680 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:43:52.852837 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_silver_users
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: user data cleaned and anonymized for analysis -- 
select
  id as user_id,
  sha1(email) as email, 
  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, 
  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, 
  initcap(firstname) as firstname, 
  initcap(lastname) as lastname, 
  address, 
  canal, 
  country,
  cast(gender as int),
  cast(age_group as int), 
  cast(churn as int) as churn
from dbdemos.dbt_c360_bronze_users
  
[0m00:43:52.852951 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:43:59.085633 [debug] [Thread-1  ]: SQL status: OK in 6.23 seconds
[0m00:43:59.088832 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:59.089016 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: ROLLBACK
[0m00:43:59.089135 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:43:59.089244 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: Close
[0m00:43:59.271296 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadf1a7f640>]}
[0m00:43:59.271685 [info ] [Thread-1  ]: 3 of 4 OK created sql table model dbdemos.dbt_c360_silver_users ................ [[32mOK[0m in 6.43s]
[0m00:43:59.272031 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:43:59.272615 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:43:59.272899 [info ] [Thread-1  ]: 4 of 4 START sql table model dbdemos.dbt_c360_gold_churn_features .............. [RUN]
[0m00:43:59.273439 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:43:59.273621 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:43:59.273857 [debug] [Thread-1  ]: Compiling model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:43:59.277824 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:43:59.278883 [debug] [Thread-1  ]: finished collecting timing info
[0m00:43:59.279108 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:43:59.283465 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:43:59.283926 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:43:59.284096 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:43:59.284268 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.3.1", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.2.1", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

  
    
        create or replace table asong_dev.dbdemos.dbt_c360_gold_churn_features
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: final user table with all information for Analysis / ML -- 
with 
    -- block 1 -- 
    churn_orders_stats as 
    (select user_id, 
            count(*) as order_count, 
            sum(amount) as total_amount, 
            sum(item_count) as total_item, 
             max(creation_date) as last_transaction
      from asong_dev.dbdemos.dbt_c360_silver_orders 
      group by user_id
    ),  
    -- block 2 -- 
    churn_app_events_stats as 
    (
      select first(platform) as platform, 
             user_id, 
             count(*) as event_count, 
             count(distinct session_id) as session_count, 
             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event
       from asong_dev.dbdemos.dbt_c360_silver_events 
       group by user_id
    )

select *, 
       datediff(now(), creation_date) as days_since_creation,
       datediff(now(), last_activity_date) as days_since_last_activity,
       datediff(now(), last_event) as days_last_event
from asong_dev.dbdemos.dbt_c360_silver_users 
inner join churn_orders_stats using (user_id)
inner join churn_app_events_stats using (user_id)
  
[0m00:43:59.284380 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:44:04.435944 [debug] [Thread-1  ]: SQL status: OK in 5.15 seconds
[0m00:44:04.438433 [debug] [Thread-1  ]: finished collecting timing info
[0m00:44:04.438595 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: ROLLBACK
[0m00:44:04.438709 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:44:04.438815 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: Close
[0m00:44:07.948688 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9da3de96-46ac-4331-b5b4-62934fdc94c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae11150dc0>]}
[0m00:44:07.949098 [info ] [Thread-1  ]: 4 of 4 OK created sql table model dbdemos.dbt_c360_gold_churn_features ......... [[32mOK[0m in 8.68s]
[0m00:44:07.949416 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:44:07.950602 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m00:44:07.950818 [debug] [MainThread]: On master: ROLLBACK
[0m00:44:07.950936 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:44:08.896691 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:44:08.896985 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:44:08.897104 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:44:08.897226 [debug] [MainThread]: On master: ROLLBACK
[0m00:44:08.897325 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:44:08.897422 [debug] [MainThread]: On master: Close
[0m00:44:09.804063 [info ] [MainThread]: 
[0m00:44:09.804565 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 57.71 seconds (57.71s).
[0m00:44:09.804801 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:44:09.805127 [debug] [MainThread]: Connection 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features' was properly closed.
[0m00:44:09.813696 [info ] [MainThread]: 
[0m00:44:09.813983 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:44:09.814212 [info ] [MainThread]: 
[0m00:44:09.814494 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m00:44:09.814778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae111318b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae11131670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadc00c20a0>]}
[0m00:44:09.814981 [debug] [MainThread]: Flushing usage events
[0m21:41:40.633778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff93a04190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff93a04c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff93a04450>]}


============================== 21:41:40.638660 | 42a373a6-61c4-4e14-9ea6-c4de4aff3957 ==============================
[0m21:41:40.638660 [info ] [MainThread]: Running with dbt=1.5.3
[0m21:41:40.639223 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:41:40.644032 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DBT_DATABRICKS_HOST'
[0m21:41:40.644749 [debug] [MainThread]: Command `dbt run` failed at 21:41:40.644630 after 0.02 seconds
[0m21:41:40.645119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff93b52c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff95d2d610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff93a16a10>]}
[0m21:41:40.645500 [debug] [MainThread]: Flushing usage events
[0m21:47:44.780419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732710>]}


============================== 21:47:44.785888 | ac5c0074-885e-424e-aed0-2c19f094d217 ==============================
[0m21:47:44.785888 [info ] [MainThread]: Running with dbt=1.5.3
[0m21:47:44.786529 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:47:44.792192 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DBT_DATABRICKS_HOST'
[0m21:47:44.793054 [debug] [MainThread]: Command `dbt run` failed at 21:47:44.792921 after 0.02 seconds
[0m21:47:44.793481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa7732c10>]}
[0m21:47:44.793908 [debug] [MainThread]: Flushing usage events
[0m21:53:07.802362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16c27d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16c2e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16c2110>]}


============================== 21:53:07.806803 | bb6a5175-7fdd-4b6a-95a5-622ff200ddae ==============================
[0m21:53:07.806803 [info ] [MainThread]: Running with dbt=1.5.3
[0m21:53:07.807509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:53:08.268024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16d5dd0>]}
[0m21:53:08.277658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9a407c90>]}
[0m21:53:08.278246 [info ] [MainThread]: Registered adapter: databricks=1.5.5
[0m21:53:08.288778 [debug] [MainThread]: checksum: d91324b50612e0c4823cec6657ec795d80c11aba9d5b50e83fb26de63277dec4, vars: {}, profile: , target: , version: 1.5.3
[0m21:53:08.297026 [debug] [MainThread]: Failed to load parsed file from disk at /opt/airflow/dbt/target/partial_parse.msgpack: Field "nodes" of type MutableMapping[str, Union[AnalysisNode, SingularTestNode, HookNode, ModelNode, RPCNode, SqlNode, GenericTestNode, SnapshotNode, SeedNode]] in Manifest has invalid value {'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features': {'resource_type': 'model', 'depends_on': {'macros': [], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_orders', 'model.dbdemos_dbt_c360.dbt_c360_silver_events', 'model.dbdemos_dbt_c360.dbt_c360_silver_users']}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'file_format': 'delta', 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'dbt_c360_gold_churn_features'], 'unique_id': 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features', 'raw_code': '{{\n config(materialized = \'table\', file_format = \'delta\')\n}}\n\n-- notes: final user table with all information for Analysis / ML -- \nwith \n    -- block 1 -- \n    churn_orders_stats as \n    (select user_id, \n            count(*) as order_count, \n            sum(amount) as total_amount, \n            sum(item_count) as total_item, \n             max(creation_date) as last_transaction\n      from {{ref(\'dbt_c360_silver_orders\')}} \n      group by user_id\n    ),  \n    -- block 2 -- \n    churn_app_events_stats as \n    (\n      select first(platform) as platform, \n             user_id, \n             count(*) as event_count, \n             count(distinct session_id) as session_count, \n             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event\n       from {{ref(\'dbt_c360_silver_events\')}} \n       group by user_id\n    )\n\nselect *, \n       datediff(now(), creation_date) as days_since_creation,\n       datediff(now(), last_activity_date) as days_since_last_activity,\n       datediff(now(), last_event) as days_last_event\nfrom {{ref(\'dbt_c360_silver_users\')}} \ninner join churn_orders_stats using (user_id)\ninner join churn_app_events_stats using (user_id)', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'dbt_c360_gold_churn_features.sql', 'original_file_path': 'models/dbt_c360_gold_churn_features.sql', 'name': 'dbt_c360_gold_churn_features', 'alias': 'dbt_c360_gold_churn_features', 'checksum': {'name': 'sha256', 'checksum': '396a206d74134d2899ab1591f88c87072337d8a59b90c367d26a85daf166a173'}, 'tags': [], 'refs': [['dbt_c360_silver_orders'], ['dbt_c360_silver_events'], ['dbt_c360_silver_users']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table', 'file_format': 'delta'}, 'created_at': 1689658004.7019231, 'config_call_dict': {'materialized': 'table', 'file_format': 'delta'}}, 'model.dbdemos_dbt_c360.dbt_c360_silver_events': {'resource_type': 'model', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'file_format': 'delta', 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'dbt_c360_silver_events'], 'unique_id': 'model.dbdemos_dbt_c360.dbt_c360_silver_events', 'raw_code': "{{\n config(materialized = 'table', file_format = 'delta')\n}}\n\nselect \n  user_id,\n  session_id,\n  event_id,\n  `date`,\n  platform,\n  action,\n  url\nfrom dbdemos.dbt_c360_bronze_events", 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'dbt_c360_silver_events.sql', 'original_file_path': 'models/dbt_c360_silver_events.sql', 'name': 'dbt_c360_silver_events', 'alias': 'dbt_c360_silver_events', 'checksum': {'name': 'sha256', 'checksum': '67ccd4773bfebe1d0fa50fde85957bd63edec5d6712d2d85a983b1d2d077961f'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table', 'file_format': 'delta'}, 'created_at': 1689658004.712426, 'config_call_dict': {'materialized': 'table', 'file_format': 'delta'}}, 'model.dbdemos_dbt_c360.dbt_c360_silver_users': {'resource_type': 'model', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'file_format': 'delta', 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'dbt_c360_silver_users'], 'unique_id': 'model.dbdemos_dbt_c360.dbt_c360_silver_users', 'raw_code': '{{\n config(materialized = \'table\', file_format = \'delta\')\n}}\n\n-- notes: user data cleaned and anonymized for analysis -- \nselect\n  id as user_id,\n  sha1(email) as email, \n  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, \n  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, \n  initcap(firstname) as firstname, \n  initcap(lastname) as lastname, \n  address, \n  canal, \n  country,\n  cast(gender as int),\n  cast(age_group as int), \n  cast(churn as int) as churn\nfrom dbdemos.dbt_c360_bronze_users', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'dbt_c360_silver_users.sql', 'original_file_path': 'models/dbt_c360_silver_users.sql', 'name': 'dbt_c360_silver_users', 'alias': 'dbt_c360_silver_users', 'checksum': {'name': 'sha256', 'checksum': 'a44ed9a40fadbf3c2cba5a5bed450cebbba219983aa07a5f6abf551114d6f78e'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {'user_id': {'name': 'user_id', 'description': '', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}, 'churn': {'name': 'churn', 'description': '', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': 'dbdemos_dbt_c360://models/schema.yml', 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table', 'file_format': 'delta'}, 'created_at': 1689658004.7642, 'config_call_dict': {'materialized': 'table', 'file_format': 'delta'}}, 'model.dbdemos_dbt_c360.dbt_c360_silver_orders': {'resource_type': 'model', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'file_format': 'delta', 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'dbt_c360_silver_orders'], 'unique_id': 'model.dbdemos_dbt_c360.dbt_c360_silver_orders', 'raw_code': '{{\n config(materialized = \'table\', file_format = \'delta\')\n}}\n\n--notes: order data cleaned and anonymized for analysis -- \nselect\n  cast(amount as int),\n  `id` as order_id,\n  user_id,\n  cast(item_count as int),\n  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date\nfrom dbdemos.dbt_c360_bronze_orders', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'dbt_c360_silver_orders.sql', 'original_file_path': 'models/dbt_c360_silver_orders.sql', 'name': 'dbt_c360_silver_orders', 'alias': 'dbt_c360_silver_orders', 'checksum': {'name': 'sha256', 'checksum': '77d4d3e61c1522876312935ce16225b19f4554dd04967244de96d23fca705256'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {'order_id': {'name': 'order_id', 'description': '', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}, 'user_id': {'name': 'user_id', 'description': '', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': 'dbdemos_dbt_c360://models/schema.yml', 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table', 'file_format': 'delta'}, 'created_at': 1689658004.7651079, 'config_call_dict': {'materialized': 'table', 'file_format': 'delta'}}, 'test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero': {'resource_type': 'test', 'depends_on': {'macros': [], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_orders']}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': True, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'assert_orders_amount_must_be_above_zero'], 'unique_id': 'test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero', 'raw_code': "{{ config(store_failures = true) }}\n\n-- notes: quarantine records and isolate them if the total sales amount is negative -- \nselect \n user_id,\n sum(amount) as total_amount \nfrom {{ref('dbt_c360_silver_orders')}}\ngroup by 1 \nhaving not (total_amount >= 0)", 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'assert_orders_amount_must_be_above_zero.sql', 'original_file_path': 'tests/assert_orders_amount_must_be_above_zero.sql', 'name': 'assert_orders_amount_must_be_above_zero', 'alias': 'assert_orders_amount_must_be_above_zero', 'checksum': {'name': 'sha256', 'checksum': '7e7580b44591ae0f0318e580fbc487be4844cbc8867a39f9050375f74e74fee4'}, 'tags': [], 'refs': [['dbt_c360_silver_orders']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'store_failures': True}, 'created_at': 1689658004.726936, 'config_call_dict': {'store_failures': True}}, 'seed.dbdemos_dbt_c360.raw_clickstream': {'resource_type': 'seed', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'seed', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'quote_columns': None, 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'raw_clickstream'], 'unique_id': 'seed.dbdemos_dbt_c360.raw_clickstream', 'raw_code': '', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'raw_clickstream.csv', 'original_file_path': 'seeds/raw_clickstream.csv', 'name': 'raw_clickstream', 'alias': 'raw_clickstream', 'checksum': {'name': 'path', 'checksum': 'seeds/raw_clickstream.csv'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.7397308, 'config_call_dict': {}}, 'seed.dbdemos_dbt_c360.raw_orders': {'resource_type': 'seed', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'seed', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'quote_columns': None, 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'raw_orders'], 'unique_id': 'seed.dbdemos_dbt_c360.raw_orders', 'raw_code': '', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'raw_orders.csv', 'original_file_path': 'seeds/raw_orders.csv', 'name': 'raw_orders', 'alias': 'raw_orders', 'checksum': {'name': 'path', 'checksum': 'seeds/raw_orders.csv'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.7417102, 'config_call_dict': {}}, 'seed.dbdemos_dbt_c360.raw_users': {'resource_type': 'seed', 'depends_on': {'macros': [], 'nodes': []}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'seed', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'quote_columns': None, 'post-hook': [], 'pre-hook': []}, 'database': 'asong_dev', 'schema': 'dbdemos', 'fqn': ['dbdemos_dbt_c360', 'raw_users'], 'unique_id': 'seed.dbdemos_dbt_c360.raw_users', 'raw_code': '', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'raw_users.csv', 'original_file_path': 'seeds/raw_users.csv', 'name': 'raw_users', 'alias': 'raw_users', 'checksum': {'name': 'path', 'checksum': 'seeds/raw_users.csv'}, 'tags': [], 'refs': [], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.74364, 'config_call_dict': {}}, 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63': {'test_metadata': {'name': 'unique', 'kwargs': {'column_name': 'user_id', 'model': "{{ get_where_subquery(ref('dbt_c360_silver_users')) }}"}, 'namespace': None}, 'resource_type': 'test', 'depends_on': {'macros': ['macro.dbt.test_unique'], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_users']}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'unique_dbt_c360_silver_users_user_id'], 'unique_id': 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63', 'raw_code': '{{ test_unique(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'unique_dbt_c360_silver_users_user_id.sql', 'original_file_path': 'models/schema.yml', 'name': 'unique_dbt_c360_silver_users_user_id', 'alias': 'unique_dbt_c360_silver_users_user_id', 'checksum': {'name': 'none', 'checksum': ''}, 'tags': [], 'refs': [['dbt_c360_silver_users']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.7699049, 'config_call_dict': {}, 'column_name': 'user_id', 'file_key_name': 'models.dbt_c360_silver_users'}, 'test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5': {'test_metadata': {'name': 'not_null', 'kwargs': {'column_name': 'user_id', 'model': "{{ get_where_subquery(ref('dbt_c360_silver_users')) }}"}, 'namespace': None}, 'resource_type': 'test', 'depends_on': {'macros': ['macro.dbt.test_not_null'], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_users']}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'not_null_dbt_c360_silver_users_user_id'], 'unique_id': 'test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5', 'raw_code': '{{ test_not_null(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'not_null_dbt_c360_silver_users_user_id.sql', 'original_file_path': 'models/schema.yml', 'name': 'not_null_dbt_c360_silver_users_user_id', 'alias': 'not_null_dbt_c360_silver_users_user_id', 'checksum': {'name': 'none', 'checksum': ''}, 'tags': [], 'refs': [['dbt_c360_silver_users']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.771723, 'config_call_dict': {}, 'column_name': 'user_id', 'file_key_name': 'models.dbt_c360_silver_users'}, 'test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3': {'test_metadata': {'name': 'accepted_values', 'kwargs': {'values': [1, 0], 'column_name': 'churn', 'model': "{{ get_where_subquery(ref('dbt_c360_silver_users')) }}"}, 'namespace': None}, 'resource_type': 'test', 'depends_on': {'macros': ['macro.dbt.test_accepted_values', 'macro.dbt.get_where_subquery'], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_users']}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'accepted_values_dbt_c360_silver_users_churn__1__0'], 'unique_id': 'test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3', 'raw_code': '{{ test_accepted_values(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'accepted_values_dbt_c360_silver_users_churn__1__0.sql', 'original_file_path': 'models/schema.yml', 'name': 'accepted_values_dbt_c360_silver_users_churn__1__0', 'alias': 'accepted_values_dbt_c360_silver_users_churn__1__0', 'checksum': {'name': 'none', 'checksum': ''}, 'tags': [], 'refs': [['dbt_c360_silver_users']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.7734458, 'config_call_dict': {}, 'column_name': 'churn', 'file_key_name': 'models.dbt_c360_silver_users'}, 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9': {'test_metadata': {'name': 'unique', 'kwargs': {'column_name': 'order_id', 'model': "{{ get_where_subquery(ref('dbt_c360_silver_orders')) }}"}, 'namespace': None}, 'resource_type': 'test', 'depends_on': {'macros': ['macro.dbt.test_unique'], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_orders']}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'unique_dbt_c360_silver_orders_order_id'], 'unique_id': 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9', 'raw_code': '{{ test_unique(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'unique_dbt_c360_silver_orders_order_id.sql', 'original_file_path': 'models/schema.yml', 'name': 'unique_dbt_c360_silver_orders_order_id', 'alias': 'unique_dbt_c360_silver_orders_order_id', 'checksum': {'name': 'none', 'checksum': ''}, 'tags': [], 'refs': [['dbt_c360_silver_orders']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1689658004.7828789, 'config_call_dict': {}, 'column_name': 'order_id', 'file_key_name': 'models.dbt_c360_silver_orders'}, 'test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838': {'test_metadata': {'name': 'relationships', 'kwargs': {'to': "ref('dbt_c360_silver_users')", 'field': 'user_id', 'column_name': 'user_id', 'model': "{{ get_where_subquery(ref('dbt_c360_silver_orders')) }}"}, 'namespace': None}, 'resource_type': 'test', 'depends_on': {'macros': ['macro.dbt.test_relationships', 'macro.dbt.get_where_subquery'], 'nodes': ['model.dbdemos_dbt_c360.dbt_c360_silver_users', 'model.dbdemos_dbt_c360.dbt_c360_silver_orders']}, 'config': {'enabled': True, 'alias': 'relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9', 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'database': 'asong_dev', 'schema': 'dbdemos_dbt_test__audit', 'fqn': ['dbdemos_dbt_c360', 'relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_'], 'unique_id': 'test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838', 'raw_code': '{{ test_relationships(**_dbt_generic_test_kwargs) }}{{ config(alias="relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9") }}', 'language': 'sql', 'package_name': 'dbdemos_dbt_c360', 'root_path': '/Users/april.song/Documents/demos/dbt_demo/dbt-databricks-c360', 'path': 'relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9.sql', 'original_file_path': 'models/schema.yml', 'name': 'relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_', 'alias': 'relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9', 'checksum': {'name': 'none', 'checksum': ''}, 'tags': [], 'refs': [['dbt_c360_silver_users'], ['dbt_c360_silver_orders']], 'sources': [], 'metrics': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'compiled_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'alias': 'relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9'}, 'created_at': 1689658004.7846882, 'config_call_dict': {'alias': 'relationships_dbt_c360_silver__63884746cb367ba27b620c2b610997b9'}, 'column_name': 'user_id', 'file_key_name': 'models.dbt_c360_silver_orders'}}
[0m21:53:08.298294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9a456f90>]}
[0m21:53:08.914099 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_gold_churn_features.sql
[0m21:53:08.923968 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_events.sql
[0m21:53:08.927167 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_users.sql
[0m21:53:08.930400 [debug] [MainThread]: 1699: static parser successfully parsed dbt_c360_silver_orders.sql
[0m21:53:09.042672 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging
[0m21:53:09.047366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff999da850>]}
[0m21:53:09.055799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9a3ad650>]}
[0m21:53:09.056339 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m21:53:09.056889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9988f710>]}
[0m21:53:09.058041 [info ] [MainThread]: 
[0m21:53:09.058811 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:53:09.059847 [debug] [ThreadPool]: Acquiring new databricks connection 'list_asong_dev'
[0m21:53:09.060253 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m21:53:09.060621 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=`asong_dev`, schema=None)
[0m21:53:09.060953 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:09.297704 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-a648-1291-907a-a315ee6c5e77
[0m21:53:10.189901 [debug] [ThreadPool]: SQL status: OK in 1.1299999952316284 seconds
[0m21:53:10.198095 [debug] [ThreadPool]: On list_asong_dev: Close
[0m21:53:10.199083 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-a648-1291-907a-a315ee6c5e77
[0m21:53:10.274401 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev, now create_asong_dev_dbdemos)
[0m21:53:10.276315 [debug] [ThreadPool]: Creating schema "database: "asong_dev"
schema: "dbdemos"
"
[0m21:53:10.290557 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:10.291358 [debug] [ThreadPool]: Using databricks connection "create_asong_dev_dbdemos"
[0m21:53:10.291980 [debug] [ThreadPool]: On create_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "create_asong_dev_dbdemos"} */
create schema if not exists `asong_dev`.`dbdemos`
  
[0m21:53:10.292627 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:10.517284 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-a6ff-1bba-81d6-94f0bc07afb7
[0m21:53:10.730101 [debug] [ThreadPool]: SQL status: OK in 0.4399999976158142 seconds
[0m21:53:10.733018 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:53:10.734392 [debug] [ThreadPool]: On create_asong_dev_dbdemos: ROLLBACK
[0m21:53:10.735649 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:53:10.736871 [debug] [ThreadPool]: On create_asong_dev_dbdemos: Close
[0m21:53:10.743086 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-a6ff-1bba-81d6-94f0bc07afb7
[0m21:53:10.815926 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_asong_dev_dbdemos, now list_asong_dev_dbdemos)
[0m21:53:10.830650 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m21:53:10.831655 [debug] [ThreadPool]: On list_asong_dev_dbdemos: GetTables(database=asong_dev, schema=dbdemos, identifier=None)
[0m21:53:10.832274 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:11.120186 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-a75d-1c05-9d51-997d0941ab57
[0m21:53:11.840582 [debug] [ThreadPool]: SQL status: OK in 1.0099999904632568 seconds
[0m21:53:11.846750 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m21:53:11.847753 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-a75d-1c05-9d51-997d0941ab57
[0m21:53:11.957613 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos, now list_asong_dev_dbdemos_dbt_test__audit)
[0m21:53:11.969712 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m21:53:11.971033 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: GetTables(database=asong_dev, schema=dbdemos_dbt_test__audit, identifier=None)
[0m21:53:11.972186 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:12.176140 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-a7ff-1311-b81c-a785994c90ca
[0m21:53:12.946328 [debug] [ThreadPool]: SQL status: OK in 0.9700000286102295 seconds
[0m21:53:12.952979 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m21:53:12.954064 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-a7ff-1311-b81c-a785994c90ca
[0m21:53:13.016676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff999c4b10>]}
[0m21:53:13.025038 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:13.026018 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:13.027725 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m21:53:13.028939 [info ] [MainThread]: 
[0m21:53:13.036676 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m21:53:13.037701 [info ] [Thread-1  ]: 1 of 4 START sql table model dbdemos.dbt_c360_silver_events .................... [RUN]
[0m21:53:13.039162 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos_dbt_test__audit, now model.dbdemos_dbt_c360.dbt_c360_silver_events)
[0m21:53:13.039907 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m21:53:13.046323 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m21:53:13.050438 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_events (compile): 21:53:13.040451 => 21:53:13.050134
[0m21:53:13.051151 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m21:53:13.068838 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:13.069450 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m21:53:13.069934 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_events`
  
[0m21:53:13.070469 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:13.297808 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-a8aa-1900-9790-94a717ff8455
[0m21:53:13.711863 [debug] [Thread-1  ]: SQL status: OK in 0.6399999856948853 seconds
[0m21:53:13.769618 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m21:53:13.772286 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m21:53:13.772784 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_events`
      
      
    using delta
      
      
      
      
      
      
      as
      

select 
  user_id,
  session_id,
  event_id,
  `date`,
  platform,
  action,
  url
from dbdemos.dbt_c360_bronze_events
  
[0m21:53:17.156703 [debug] [Thread-1  ]: SQL status: OK in 3.380000114440918 seconds
[0m21:53:17.205950 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_events (execute): 21:53:13.051666 => 21:53:17.205746
[0m21:53:17.206683 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: ROLLBACK
[0m21:53:17.207142 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:17.207601 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: Close
[0m21:53:17.208089 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-a8aa-1900-9790-94a717ff8455
[0m21:53:17.271067 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98dbfc10>]}
[0m21:53:17.272423 [info ] [Thread-1  ]: 1 of 4 OK created sql table model dbdemos.dbt_c360_silver_events ............... [[32mOK[0m in 4.23s]
[0m21:53:17.273582 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m21:53:17.274448 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m21:53:17.275204 [info ] [Thread-1  ]: 2 of 4 START sql table model dbdemos.dbt_c360_silver_orders .................... [RUN]
[0m21:53:17.276425 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_events, now model.dbdemos_dbt_c360.dbt_c360_silver_orders)
[0m21:53:17.277058 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m21:53:17.280700 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m21:53:17.282962 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_orders (compile): 21:53:17.277585 => 21:53:17.282754
[0m21:53:17.283581 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m21:53:17.289974 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:17.290681 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m21:53:17.291276 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
  
[0m21:53:17.291937 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:17.502641 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-ab2c-1574-8c14-a1a34d52efd0
[0m21:53:17.860318 [debug] [Thread-1  ]: SQL status: OK in 0.5699999928474426 seconds
[0m21:53:17.880853 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m21:53:17.885052 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m21:53:17.885772 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      

--notes: order data cleaned and anonymized for analysis -- 
select
  cast(amount as int),
  `id` as order_id,
  user_id,
  cast(item_count as int),
  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date
from dbdemos.dbt_c360_bronze_orders
  
[0m21:53:21.161390 [debug] [Thread-1  ]: SQL status: OK in 3.2699999809265137 seconds
[0m21:53:21.166051 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_orders (execute): 21:53:17.284075 => 21:53:21.165886
[0m21:53:21.166641 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: ROLLBACK
[0m21:53:21.167138 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:21.167731 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: Close
[0m21:53:21.168249 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-ab2c-1574-8c14-a1a34d52efd0
[0m21:53:21.226995 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98f02450>]}
[0m21:53:21.228081 [info ] [Thread-1  ]: 2 of 4 OK created sql table model dbdemos.dbt_c360_silver_orders ............... [[32mOK[0m in 3.95s]
[0m21:53:21.229158 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m21:53:21.229840 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m21:53:21.230516 [info ] [Thread-1  ]: 3 of 4 START sql table model dbdemos.dbt_c360_silver_users ..................... [RUN]
[0m21:53:21.231593 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_orders, now model.dbdemos_dbt_c360.dbt_c360_silver_users)
[0m21:53:21.232179 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m21:53:21.235573 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m21:53:21.238489 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_users (compile): 21:53:21.232629 => 21:53:21.238310
[0m21:53:21.238982 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m21:53:21.243604 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:21.244280 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m21:53:21.244814 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
  
[0m21:53:21.245323 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:21.409237 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-ad80-1413-a703-1b86e225dfd9
[0m21:53:21.788803 [debug] [Thread-1  ]: SQL status: OK in 0.5400000214576721 seconds
[0m21:53:21.798212 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m21:53:21.802073 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m21:53:21.802900 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: user data cleaned and anonymized for analysis -- 
select
  id as user_id,
  sha1(email) as email, 
  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, 
  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, 
  initcap(firstname) as firstname, 
  initcap(lastname) as lastname, 
  address, 
  canal, 
  country,
  cast(gender as int),
  cast(age_group as int), 
  cast(churn as int) as churn
from dbdemos.dbt_c360_bronze_users
  
[0m21:53:25.861386 [debug] [Thread-1  ]: SQL status: OK in 4.059999942779541 seconds
[0m21:53:25.871039 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_users (execute): 21:53:21.239384 => 21:53:25.870694
[0m21:53:25.872362 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: ROLLBACK
[0m21:53:25.873613 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:25.875105 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: Close
[0m21:53:25.876445 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-ad80-1413-a703-1b86e225dfd9
[0m21:53:25.942005 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98ebb250>]}
[0m21:53:25.944518 [info ] [Thread-1  ]: 3 of 4 OK created sql table model dbdemos.dbt_c360_silver_users ................ [[32mOK[0m in 4.71s]
[0m21:53:25.948050 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m21:53:25.952102 [debug] [Thread-1  ]: Began running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m21:53:25.954755 [info ] [Thread-1  ]: 4 of 4 START sql table model dbdemos.dbt_c360_gold_churn_features .............. [RUN]
[0m21:53:25.959039 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_users, now model.dbdemos_dbt_c360.dbt_c360_gold_churn_features)
[0m21:53:25.961282 [debug] [Thread-1  ]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m21:53:25.968664 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m21:53:25.972129 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_gold_churn_features (compile): 21:53:25.962967 => 21:53:25.971806
[0m21:53:25.973083 [debug] [Thread-1  ]: Began executing node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m21:53:25.978764 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:25.979487 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m21:53:25.980315 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_gold_churn_features`
  
[0m21:53:25.981186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:26.194726 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b05a-12aa-8fcf-a95c35101dbc
[0m21:53:26.592726 [debug] [Thread-1  ]: SQL status: OK in 0.6100000143051147 seconds
[0m21:53:26.599371 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m21:53:26.601992 [debug] [Thread-1  ]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m21:53:26.602575 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_gold_churn_features`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: final user table with all information for Analysis / ML -- 
with 
    -- block 1 -- 
    churn_orders_stats as 
    (select user_id, 
            count(*) as order_count, 
            sum(amount) as total_amount, 
            sum(item_count) as total_item, 
             max(creation_date) as last_transaction
      from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders` 
      group by user_id
    ),  
    -- block 2 -- 
    churn_app_events_stats as 
    (
      select first(platform) as platform, 
             user_id, 
             count(*) as event_count, 
             count(distinct session_id) as session_count, 
             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event
       from `asong_dev`.`dbdemos`.`dbt_c360_silver_events` 
       group by user_id
    )

select *, 
       datediff(now(), creation_date) as days_since_creation,
       datediff(now(), last_activity_date) as days_since_last_activity,
       datediff(now(), last_event) as days_last_event
from `asong_dev`.`dbdemos`.`dbt_c360_silver_users` 
inner join churn_orders_stats using (user_id)
inner join churn_app_events_stats using (user_id)
  
[0m21:53:30.549403 [debug] [Thread-1  ]: SQL status: OK in 3.950000047683716 seconds
[0m21:53:30.554440 [debug] [Thread-1  ]: Timing info for model.dbdemos_dbt_c360.dbt_c360_gold_churn_features (execute): 21:53:25.973712 => 21:53:30.554215
[0m21:53:30.555213 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: ROLLBACK
[0m21:53:30.555879 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:30.556469 [debug] [Thread-1  ]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: Close
[0m21:53:30.557070 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-b05a-12aa-8fcf-a95c35101dbc
[0m21:53:30.618502 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb6a5175-7fdd-4b6a-95a5-622ff200ddae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98ee4d10>]}
[0m21:53:30.619793 [info ] [Thread-1  ]: 4 of 4 OK created sql table model dbdemos.dbt_c360_gold_churn_features ......... [[32mOK[0m in 4.66s]
[0m21:53:30.621179 [debug] [Thread-1  ]: Finished running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m21:53:30.691813 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:30.692851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:30.881182 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b323-1da7-ba74-8e79de4de00f
[0m21:53:30.886037 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:30.888121 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:30.890577 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:30.892114 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:30.893499 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:30.894789 [debug] [MainThread]: On master: Close
[0m21:53:30.898382 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01ee267e-b323-1da7-ba74-8e79de4de00f
[0m21:53:30.972326 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:30.976165 [debug] [MainThread]: Connection 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features' was properly closed.
[0m21:53:30.979845 [info ] [MainThread]: 
[0m21:53:30.984219 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 21.92 seconds (21.92s).
[0m21:53:30.987895 [debug] [MainThread]: Command end result
[0m21:53:31.010459 [info ] [MainThread]: 
[0m21:53:31.011713 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:31.012697 [info ] [MainThread]: 
[0m21:53:31.013504 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m21:53:31.014437 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:31.014327 after 23.22 seconds
[0m21:53:31.015079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16c2b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16c2d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffb16dff10>]}
[0m21:53:31.015853 [debug] [MainThread]: Flushing usage events
[0m21:53:34.575019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447bd50>]}


============================== 21:53:34.579556 | 4685a393-30fd-44ef-bdeb-f23dcbd3c70b ==============================
[0m21:53:34.579556 [info ] [MainThread]: Running with dbt=1.5.3
[0m21:53:34.580108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:53:35.026065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff94497f90>]}
[0m21:53:35.035912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8113e090>]}
[0m21:53:35.036679 [info ] [MainThread]: Registered adapter: databricks=1.5.5
[0m21:53:35.048895 [debug] [MainThread]: checksum: d91324b50612e0c4823cec6657ec795d80c11aba9d5b50e83fb26de63277dec4, vars: {}, profile: , target: , version: 1.5.3
[0m21:53:35.075450 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:35.076116 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:35.076961 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging
[0m21:53:35.082086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff80f18410>]}
[0m21:53:35.089597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff81045450>]}
[0m21:53:35.090078 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m21:53:35.090562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff81045d50>]}
[0m21:53:35.091712 [info ] [MainThread]: 
[0m21:53:35.092425 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:53:35.093394 [debug] [ThreadPool]: Acquiring new databricks connection 'list_asong_dev'
[0m21:53:35.093758 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m21:53:35.094060 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=`asong_dev`, schema=None)
[0m21:53:35.094362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:35.260496 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b5c2-1087-bf7a-b83683628ccf
[0m21:53:35.899335 [debug] [ThreadPool]: SQL status: OK in 0.800000011920929 seconds
[0m21:53:35.911284 [debug] [ThreadPool]: On list_asong_dev: Close
[0m21:53:35.912392 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-b5c2-1087-bf7a-b83683628ccf
[0m21:53:35.978842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev, now create_asong_dev_dbdemos_dbt_test__audit)
[0m21:53:35.980529 [debug] [ThreadPool]: Creating schema "database: "asong_dev"
schema: "dbdemos_dbt_test__audit"
"
[0m21:53:35.992463 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:35.993187 [debug] [ThreadPool]: Using databricks connection "create_asong_dev_dbdemos_dbt_test__audit"
[0m21:53:35.993878 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "create_asong_dev_dbdemos_dbt_test__audit"} */
create schema if not exists `asong_dev`.`dbdemos_dbt_test__audit`
  
[0m21:53:35.994529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:36.200979 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b650-178e-a193-f961f260bd26
[0m21:53:36.520546 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m21:53:36.521662 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:53:36.522260 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: ROLLBACK
[0m21:53:36.522797 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:53:36.523238 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: Close
[0m21:53:36.523741 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-b650-178e-a193-f961f260bd26
[0m21:53:36.626222 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_asong_dev_dbdemos_dbt_test__audit, now list_asong_dev_dbdemos_dbt_test__audit)
[0m21:53:36.630281 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m21:53:36.630882 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: GetTables(database=asong_dev, schema=dbdemos_dbt_test__audit, identifier=None)
[0m21:53:36.631325 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:36.802460 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b6ab-1ec1-b414-b65726d5f281
[0m21:53:37.444279 [debug] [ThreadPool]: SQL status: OK in 0.8100000023841858 seconds
[0m21:53:37.447380 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m21:53:37.448106 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-b6ab-1ec1-b414-b65726d5f281
[0m21:53:37.509654 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos_dbt_test__audit, now list_asong_dev_dbdemos)
[0m21:53:37.511894 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m21:53:37.512385 [debug] [ThreadPool]: On list_asong_dev_dbdemos: GetTables(database=asong_dev, schema=dbdemos, identifier=None)
[0m21:53:37.512754 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:37.721011 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b738-1dec-95ea-503c7a74d2fc
[0m21:53:38.392961 [debug] [ThreadPool]: SQL status: OK in 0.8799999952316284 seconds
[0m21:53:38.399585 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m21:53:38.400501 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee267e-b738-1dec-95ea-503c7a74d2fc
[0m21:53:38.467223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4685a393-30fd-44ef-bdeb-f23dcbd3c70b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9505cb50>]}
[0m21:53:38.470163 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:38.472118 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:38.474447 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m21:53:38.475532 [info ] [MainThread]: 
[0m21:53:38.483445 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m21:53:38.484579 [info ] [Thread-1  ]: 1 of 6 START test accepted_values_dbt_c360_silver_users_churn__1__0 ............ [RUN]
[0m21:53:38.486130 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos, now test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3)
[0m21:53:38.487073 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m21:53:38.502697 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m21:53:38.506110 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3 (compile): 21:53:38.487847 => 21:53:38.505876
[0m21:53:38.506840 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m21:53:38.523218 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m21:53:38.525565 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:38.526062 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m21:53:38.526521 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        churn as value_field,
        count(*) as n_records

    from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
    group by churn

)

select *
from all_values
where value_field not in (
    '1','0'
)



      
    ) dbt_internal_test
[0m21:53:38.527032 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:38.726748 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b7d2-1d1a-bc9b-e20a0b5b6278
[0m21:53:39.131445 [debug] [Thread-1  ]: SQL status: OK in 0.6000000238418579 seconds
[0m21:53:39.138416 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3 (execute): 21:53:38.507417 => 21:53:39.138107
[0m21:53:39.139456 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: ROLLBACK
[0m21:53:39.140277 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:39.141029 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: Close
[0m21:53:39.141987 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-b7d2-1d1a-bc9b-e20a0b5b6278
[0m21:53:39.200894 [info ] [Thread-1  ]: 1 of 6 PASS accepted_values_dbt_c360_silver_users_churn__1__0 .................. [[32mPASS[0m in 0.72s]
[0m21:53:39.202198 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m21:53:39.202937 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m21:53:39.203597 [info ] [Thread-1  ]: 2 of 6 START test assert_orders_amount_must_be_above_zero ...................... [RUN]
[0m21:53:39.204716 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3, now test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero)
[0m21:53:39.205322 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m21:53:39.209067 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m21:53:39.211540 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero (compile): 21:53:39.205764 => 21:53:39.211333
[0m21:53:39.212101 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m21:53:39.244752 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:39.245383 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m21:53:39.245793 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"} */

        
  
    
        create or replace table `asong_dev`.`dbdemos_dbt_test__audit`.`assert_orders_amount_must_be_above_zero`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: quarantine records and isolate them if the total sales amount is negative -- 
select 
 user_id,
 sum(amount) as total_amount 
from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
group by 1 
having not (total_amount >= 0)
  
    
[0m21:53:39.246224 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:39.415270 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-b83b-1988-a75f-a9a304fa577c
[0m21:53:41.986154 [debug] [Thread-1  ]: SQL status: OK in 2.740000009536743 seconds
[0m21:53:41.991279 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m21:53:42.011515 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m21:53:42.079359 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m21:53:42.080010 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `asong_dev`.`dbdemos_dbt_test__audit`.`assert_orders_amount_must_be_above_zero`
    
      
    ) dbt_internal_test
[0m21:53:42.468994 [debug] [Thread-1  ]: SQL status: OK in 0.38999998569488525 seconds
[0m21:53:42.473628 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero (execute): 21:53:39.212527 => 21:53:42.473287
[0m21:53:42.474729 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: ROLLBACK
[0m21:53:42.476180 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:42.478006 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: Close
[0m21:53:42.480081 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-b83b-1988-a75f-a9a304fa577c
[0m21:53:42.545119 [info ] [Thread-1  ]: 2 of 6 PASS assert_orders_amount_must_be_above_zero ............................ [[32mPASS[0m in 3.34s]
[0m21:53:42.547976 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m21:53:42.550075 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m21:53:42.552064 [info ] [Thread-1  ]: 3 of 6 START test not_null_dbt_c360_silver_users_user_id ....................... [RUN]
[0m21:53:42.557719 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero, now test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5)
[0m21:53:42.560787 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m21:53:42.574349 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m21:53:42.576337 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5 (compile): 21:53:42.562071 => 21:53:42.576147
[0m21:53:42.576790 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m21:53:42.579107 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m21:53:42.580493 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:42.580865 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m21:53:42.581287 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
where user_id is null



      
    ) dbt_internal_test
[0m21:53:42.581775 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:42.753554 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-ba39-168d-b51e-8a6ae07a236c
[0m21:53:43.101230 [debug] [Thread-1  ]: SQL status: OK in 0.5199999809265137 seconds
[0m21:53:43.112463 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5 (execute): 21:53:42.577149 => 21:53:43.111883
[0m21:53:43.114111 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: ROLLBACK
[0m21:53:43.115924 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:43.117009 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: Close
[0m21:53:43.117851 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-ba39-168d-b51e-8a6ae07a236c
[0m21:53:43.180939 [info ] [Thread-1  ]: 3 of 6 PASS not_null_dbt_c360_silver_users_user_id ............................. [[32mPASS[0m in 0.62s]
[0m21:53:43.184197 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m21:53:43.186954 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m21:53:43.188855 [info ] [Thread-1  ]: 4 of 6 START test relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_  [RUN]
[0m21:53:43.191681 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5, now test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838)
[0m21:53:43.193024 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m21:53:43.206624 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m21:53:43.209331 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838 (compile): 21:53:43.194014 => 21:53:43.209007
[0m21:53:43.210267 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m21:53:43.214143 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m21:53:43.216624 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:43.217323 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m21:53:43.218032 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select user_id as from_field
    from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m21:53:43.218799 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:43.396164 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-ba9a-1379-ae24-a91da172a454
[0m21:53:44.073712 [debug] [Thread-1  ]: SQL status: OK in 0.8500000238418579 seconds
[0m21:53:44.078499 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838 (execute): 21:53:43.211069 => 21:53:44.078175
[0m21:53:44.079447 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: ROLLBACK
[0m21:53:44.083598 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:44.084436 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: Close
[0m21:53:44.085289 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-ba9a-1379-ae24-a91da172a454
[0m21:53:44.148288 [info ] [Thread-1  ]: 4 of 6 PASS relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_  [[32mPASS[0m in 0.96s]
[0m21:53:44.149814 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m21:53:44.150608 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m21:53:44.151459 [info ] [Thread-1  ]: 5 of 6 START test unique_dbt_c360_silver_orders_order_id ....................... [RUN]
[0m21:53:44.152724 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838, now test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9)
[0m21:53:44.153448 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m21:53:44.161192 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m21:53:44.163452 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9 (compile): 21:53:44.153990 => 21:53:44.163254
[0m21:53:44.163996 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m21:53:44.166673 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m21:53:44.168306 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:44.168784 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m21:53:44.169280 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m21:53:44.169818 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:44.342814 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-bb2a-18ee-b35a-4632ff4b2883
[0m21:53:45.014715 [debug] [Thread-1  ]: SQL status: OK in 0.8399999737739563 seconds
[0m21:53:45.017317 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9 (execute): 21:53:44.164436 => 21:53:45.017117
[0m21:53:45.017978 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: ROLLBACK
[0m21:53:45.018548 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:45.019125 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: Close
[0m21:53:45.019710 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-bb2a-18ee-b35a-4632ff4b2883
[0m21:53:45.082239 [info ] [Thread-1  ]: 5 of 6 PASS unique_dbt_c360_silver_orders_order_id ............................. [[32mPASS[0m in 0.93s]
[0m21:53:45.083412 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m21:53:45.084118 [debug] [Thread-1  ]: Began running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m21:53:45.084694 [info ] [Thread-1  ]: 6 of 6 START test unique_dbt_c360_silver_users_user_id ......................... [RUN]
[0m21:53:45.085632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9, now test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63)
[0m21:53:45.086160 [debug] [Thread-1  ]: Began compiling node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m21:53:45.090298 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m21:53:45.092369 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63 (compile): 21:53:45.086503 => 21:53:45.092137
[0m21:53:45.092990 [debug] [Thread-1  ]: Began executing node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m21:53:45.097021 [debug] [Thread-1  ]: Writing runtime sql for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m21:53:45.100297 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:45.100800 [debug] [Thread-1  ]: Using databricks connection "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m21:53:45.101359 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m21:53:45.101941 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:45.262643 [info ] [Thread-1  ]: databricks-sql-connector adapter: Successfully opened session 01ee267e-bbb7-15a3-9387-e1214c3ca057
[0m21:53:45.834899 [debug] [Thread-1  ]: SQL status: OK in 0.7300000190734863 seconds
[0m21:53:45.838032 [debug] [Thread-1  ]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63 (execute): 21:53:45.093477 => 21:53:45.837821
[0m21:53:45.838684 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: ROLLBACK
[0m21:53:45.839304 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:53:45.839851 [debug] [Thread-1  ]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: Close
[0m21:53:45.840443 [info ] [Thread-1  ]: databricks-sql-connector adapter: Closing session 01ee267e-bbb7-15a3-9387-e1214c3ca057
[0m21:53:45.908443 [info ] [Thread-1  ]: 6 of 6 PASS unique_dbt_c360_silver_users_user_id ............................... [[32mPASS[0m in 0.82s]
[0m21:53:45.909785 [debug] [Thread-1  ]: Finished running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m21:53:45.940970 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:45.942024 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:46.119654 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01ee267e-bc3a-12d2-98b8-38000c2cc059
[0m21:53:46.122898 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:46.124304 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:46.125393 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:46.126643 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:46.127905 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:46.130599 [debug] [MainThread]: On master: Close
[0m21:53:46.133057 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01ee267e-bc3a-12d2-98b8-38000c2cc059
[0m21:53:46.215186 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:46.216970 [debug] [MainThread]: Connection 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63' was properly closed.
[0m21:53:46.220058 [info ] [MainThread]: 
[0m21:53:46.222437 [info ] [MainThread]: Finished running 6 tests in 0 hours 0 minutes and 11.13 seconds (11.13s).
[0m21:53:46.226285 [debug] [MainThread]: Command end result
[0m21:53:46.247437 [info ] [MainThread]: 
[0m21:53:46.248739 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:46.249808 [info ] [MainThread]: 
[0m21:53:46.250921 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m21:53:46.252213 [debug] [MainThread]: Command `dbt test` succeeded at 21:53:46.252063 after 11.68 seconds
[0m21:53:46.252955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447b150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447b950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff9447b0d0>]}
[0m21:53:46.253653 [debug] [MainThread]: Flushing usage events
[0m23:51:39.427678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff97ec7ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff967bc490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff967bc190>]}


============================== 23:51:39.431704 | d29489f6-a342-4bce-8c10-8a653d94218e ==============================
[0m23:51:39.431704 [info ] [MainThread]: Running with dbt=1.5.3
[0m23:51:39.432417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/dbt', 'log_path': '/usr/local/airflow/dags/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m23:51:39.436706 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DBT_DATABRICKS_HOST'
[0m23:51:39.437389 [debug] [MainThread]: Command `dbt run` failed at 23:51:39.437258 after 0.02 seconds
[0m23:51:39.437798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff97ec7ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98627e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff98625f60>]}
[0m23:51:39.438190 [debug] [MainThread]: Flushing usage events
[0m00:04:29.994364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff84457dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff82d44400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff82d44100>]}


============================== 00:04:29.998407 | f78716e2-cea0-42d6-93a6-5f61decf9c4f ==============================
[0m00:04:29.998407 [info ] [MainThread]: Running with dbt=1.5.3
[0m00:04:29.999090 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/dbt', 'log_path': '/usr/local/airflow/dags/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:04:30.003711 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DBT_DATABRICKS_HOST'
[0m00:04:30.004586 [debug] [MainThread]: Command `dbt run` failed at 00:04:30.004467 after 0.02 seconds
[0m00:04:30.005092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff84457dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff84bb7d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff84bb5f60>]}
[0m00:04:30.005580 [debug] [MainThread]: Flushing usage events
[0m00:07:30.020391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8d593f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8be884c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8be881c0>]}


============================== 00:07:30.024451 | 87771f99-4138-4f3d-ac7e-ef94fe297ff9 ==============================
[0m00:07:30.024451 [info ] [MainThread]: Running with dbt=1.5.3
[0m00:07:30.025245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:07:30.604073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8be76170>]}
[0m00:07:30.613682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78df3370>]}
[0m00:07:30.614614 [info ] [MainThread]: Registered adapter: databricks=1.5.5
[0m00:07:30.625218 [debug] [MainThread]: checksum: d91324b50612e0c4823cec6657ec795d80c11aba9d5b50e83fb26de63277dec4, vars: {}, profile: , target: , version: 1.5.3
[0m00:07:30.649967 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:07:30.650670 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:07:30.651412 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging
[0m00:07:30.655608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78bfdf00>]}
[0m00:07:30.664763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78bd0a30>]}
[0m00:07:30.665300 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m00:07:30.665672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78bd0a00>]}
[0m00:07:30.666756 [info ] [MainThread]: 
[0m00:07:30.667581 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:07:30.668597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_asong_dev'
[0m00:07:30.669015 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m00:07:30.669352 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=`asong_dev`, schema=None)
[0m00:07:30.669657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:07:30.934435 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-6b66-1379-a7ae-3ad749e30c54
[0m00:07:31.740790 [debug] [ThreadPool]: SQL status: OK in 1.0700000524520874 seconds
[0m00:07:31.754767 [debug] [ThreadPool]: On list_asong_dev: Close
[0m00:07:31.756010 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-6b66-1379-a7ae-3ad749e30c54
[0m00:07:31.859979 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev, now create_asong_dev_dbdemos)
[0m00:07:31.862143 [debug] [ThreadPool]: Creating schema "database: "asong_dev"
schema: "dbdemos"
"
[0m00:07:31.882992 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:31.883697 [debug] [ThreadPool]: Using databricks connection "create_asong_dev_dbdemos"
[0m00:07:31.884131 [debug] [ThreadPool]: On create_asong_dev_dbdemos: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "create_asong_dev_dbdemos"} */
create schema if not exists `asong_dev`.`dbdemos`
  
[0m00:07:31.884507 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:07:32.086175 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-6c14-1920-8035-947fed989b66
[0m00:07:32.380606 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m00:07:32.384983 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:07:32.390044 [debug] [ThreadPool]: On create_asong_dev_dbdemos: ROLLBACK
[0m00:07:32.392745 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:07:32.394191 [debug] [ThreadPool]: On create_asong_dev_dbdemos: Close
[0m00:07:32.395820 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-6c14-1920-8035-947fed989b66
[0m00:07:32.492604 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_asong_dev_dbdemos, now list_asong_dev_dbdemos)
[0m00:07:32.509156 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m00:07:32.510988 [debug] [ThreadPool]: On list_asong_dev_dbdemos: GetTables(database=asong_dev, schema=dbdemos, identifier=None)
[0m00:07:32.512256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:07:32.694360 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-6c71-1c1d-bbdc-65bb09e59d68
[0m00:07:33.501171 [debug] [ThreadPool]: SQL status: OK in 0.9900000095367432 seconds
[0m00:07:33.504780 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m00:07:33.505506 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-6c71-1c1d-bbdc-65bb09e59d68
[0m00:07:33.585603 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos, now list_asong_dev_dbdemos_dbt_test__audit)
[0m00:07:33.588067 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m00:07:33.588501 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: GetTables(database=asong_dev, schema=dbdemos_dbt_test__audit, identifier=None)
[0m00:07:33.588915 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:07:33.854129 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-6d22-1ebc-b1bc-4e557f1a98a8
[0m00:07:34.646448 [debug] [ThreadPool]: SQL status: OK in 1.059999942779541 seconds
[0m00:07:34.655265 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m00:07:34.657545 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-6d22-1ebc-b1bc-4e557f1a98a8
[0m00:07:34.734776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78bd08b0>]}
[0m00:07:34.737326 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:34.738777 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:34.745013 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m00:07:34.747671 [info ] [MainThread]: 
[0m00:07:34.771698 [debug] [Thread-1 (]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:07:34.773167 [info ] [Thread-1 (]: 1 of 4 START sql table model dbdemos.dbt_c360_silver_events .................... [RUN]
[0m00:07:34.774533 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos_dbt_test__audit, now model.dbdemos_dbt_c360.dbt_c360_silver_events)
[0m00:07:34.775351 [debug] [Thread-1 (]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:07:34.779072 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:07:34.781911 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_events (compile): 00:07:34.775964 => 00:07:34.781665
[0m00:07:34.782610 [debug] [Thread-1 (]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:07:34.794963 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:34.795638 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:07:34.796155 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_events`
  
[0m00:07:34.796739 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:07:34.991238 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-6dcf-1fb4-8970-811a9af85302
[0m00:07:35.389136 [debug] [Thread-1 (]: SQL status: OK in 0.5899999737739563 seconds
[0m00:07:35.447947 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:07:35.452076 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_events"
[0m00:07:35.452783 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_events"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_events`
      
      
    using delta
      
      
      
      
      
      
      as
      

select 
  user_id,
  session_id,
  event_id,
  `date`,
  platform,
  action,
  url
from dbdemos.dbt_c360_bronze_events
  
[0m00:07:38.981884 [debug] [Thread-1 (]: SQL status: OK in 3.5299999713897705 seconds
[0m00:07:39.005431 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_events (execute): 00:07:34.783175 => 00:07:39.005278
[0m00:07:39.006196 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: ROLLBACK
[0m00:07:39.006763 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:07:39.007229 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_events: Close
[0m00:07:39.007713 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-6dcf-1fb4-8970-811a9af85302
[0m00:07:39.070373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff780ca860>]}
[0m00:07:39.071594 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dbdemos.dbt_c360_silver_events ............... [[32mOK[0m in 4.30s]
[0m00:07:39.072569 [debug] [Thread-1 (]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_events
[0m00:07:39.073292 [debug] [Thread-1 (]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:07:39.074070 [info ] [Thread-1 (]: 2 of 4 START sql table model dbdemos.dbt_c360_silver_orders .................... [RUN]
[0m00:07:39.075211 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_events, now model.dbdemos_dbt_c360.dbt_c360_silver_orders)
[0m00:07:39.076714 [debug] [Thread-1 (]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:07:39.079891 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:07:39.082004 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_orders (compile): 00:07:39.077162 => 00:07:39.081797
[0m00:07:39.082606 [debug] [Thread-1 (]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:07:39.086365 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:39.087007 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:07:39.087522 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
  
[0m00:07:39.088052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:07:39.275375 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-705e-1283-8800-b4f69eac3c57
[0m00:07:39.631886 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m00:07:39.636345 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:07:39.638065 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_orders"
[0m00:07:39.638557 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_orders"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      

--notes: order data cleaned and anonymized for analysis -- 
select
  cast(amount as int),
  `id` as order_id,
  user_id,
  cast(item_count as int),
  to_timestamp(transaction_date, "MM-dd-yyyy HH:mm:ss") as creation_date
from dbdemos.dbt_c360_bronze_orders
  
[0m00:07:43.235095 [debug] [Thread-1 (]: SQL status: OK in 3.5999999046325684 seconds
[0m00:07:43.237785 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_orders (execute): 00:07:39.083116 => 00:07:43.237663
[0m00:07:43.238373 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: ROLLBACK
[0m00:07:43.238811 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:07:43.239417 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_orders: Close
[0m00:07:43.239951 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-705e-1283-8800-b4f69eac3c57
[0m00:07:43.302129 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8d0f00d0>]}
[0m00:07:43.303132 [info ] [Thread-1 (]: 2 of 4 OK created sql table model dbdemos.dbt_c360_silver_orders ............... [[32mOK[0m in 4.23s]
[0m00:07:43.303921 [debug] [Thread-1 (]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_orders
[0m00:07:43.304499 [debug] [Thread-1 (]: Began running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:07:43.305109 [info ] [Thread-1 (]: 3 of 4 START sql table model dbdemos.dbt_c360_silver_users ..................... [RUN]
[0m00:07:43.305939 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_orders, now model.dbdemos_dbt_c360.dbt_c360_silver_users)
[0m00:07:43.306466 [debug] [Thread-1 (]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:07:43.309034 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:07:43.311587 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_users (compile): 00:07:43.306896 => 00:07:43.311453
[0m00:07:43.312078 [debug] [Thread-1 (]: Began executing node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:07:43.314837 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:43.315330 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:07:43.315733 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
  
[0m00:07:43.316190 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:07:43.491724 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-72e1-1aef-a213-0acd86c71492
[0m00:07:43.943443 [debug] [Thread-1 (]: SQL status: OK in 0.6299999952316284 seconds
[0m00:07:43.955683 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:07:43.958842 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_silver_users"
[0m00:07:43.960070 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_silver_users"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: user data cleaned and anonymized for analysis -- 
select
  id as user_id,
  sha1(email) as email, 
  to_timestamp(creation_date, "MM-dd-yyyy HH:mm:ss") as creation_date, 
  to_timestamp(last_activity_date, "MM-dd-yyyy HH:mm:ss") as last_activity_date, 
  initcap(firstname) as firstname, 
  initcap(lastname) as lastname, 
  address, 
  canal, 
  country,
  cast(gender as int),
  cast(age_group as int), 
  cast(churn as int) as churn
from dbdemos.dbt_c360_bronze_users
  
[0m00:07:48.328602 [debug] [Thread-1 (]: SQL status: OK in 4.369999885559082 seconds
[0m00:07:48.340664 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_silver_users (execute): 00:07:43.312435 => 00:07:48.340226
[0m00:07:48.342847 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: ROLLBACK
[0m00:07:48.346830 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:07:48.349686 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_silver_users: Close
[0m00:07:48.351654 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-72e1-1aef-a213-0acd86c71492
[0m00:07:48.422612 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78103940>]}
[0m00:07:48.430135 [info ] [Thread-1 (]: 3 of 4 OK created sql table model dbdemos.dbt_c360_silver_users ................ [[32mOK[0m in 5.12s]
[0m00:07:48.433506 [debug] [Thread-1 (]: Finished running node model.dbdemos_dbt_c360.dbt_c360_silver_users
[0m00:07:48.436400 [debug] [Thread-1 (]: Began running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:07:48.439606 [info ] [Thread-1 (]: 4 of 4 START sql table model dbdemos.dbt_c360_gold_churn_features .............. [RUN]
[0m00:07:48.445661 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbdemos_dbt_c360.dbt_c360_silver_users, now model.dbdemos_dbt_c360.dbt_c360_gold_churn_features)
[0m00:07:48.447469 [debug] [Thread-1 (]: Began compiling node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:07:48.465348 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:07:48.468785 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_gold_churn_features (compile): 00:07:48.449494 => 00:07:48.468531
[0m00:07:48.469640 [debug] [Thread-1 (]: Began executing node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:07:48.474758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:48.475819 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:07:48.476743 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

      describe extended `asong_dev`.`dbdemos`.`dbt_c360_gold_churn_features`
  
[0m00:07:48.477705 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:07:48.666924 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-75f7-15c0-8daf-d0c6369a9bc1
[0m00:07:49.047824 [debug] [Thread-1 (]: SQL status: OK in 0.5699999928474426 seconds
[0m00:07:49.056077 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:07:49.059423 [debug] [Thread-1 (]: Using databricks connection "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"
[0m00:07:49.061102 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "model.dbdemos_dbt_c360.dbt_c360_gold_churn_features"} */

  
    
        create or replace table `asong_dev`.`dbdemos`.`dbt_c360_gold_churn_features`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: final user table with all information for Analysis / ML -- 
with 
    -- block 1 -- 
    churn_orders_stats as 
    (select user_id, 
            count(*) as order_count, 
            sum(amount) as total_amount, 
            sum(item_count) as total_item, 
             max(creation_date) as last_transaction
      from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders` 
      group by user_id
    ),  
    -- block 2 -- 
    churn_app_events_stats as 
    (
      select first(platform) as platform, 
             user_id, 
             count(*) as event_count, 
             count(distinct session_id) as session_count, 
             max(to_timestamp(date, "MM-dd-yyyy HH:mm:ss")) as last_event
       from `asong_dev`.`dbdemos`.`dbt_c360_silver_events` 
       group by user_id
    )

select *, 
       datediff(now(), creation_date) as days_since_creation,
       datediff(now(), last_activity_date) as days_since_last_activity,
       datediff(now(), last_event) as days_last_event
from `asong_dev`.`dbdemos`.`dbt_c360_silver_users` 
inner join churn_orders_stats using (user_id)
inner join churn_app_events_stats using (user_id)
  
[0m00:07:53.559414 [debug] [Thread-1 (]: SQL status: OK in 4.5 seconds
[0m00:07:53.572811 [debug] [Thread-1 (]: Timing info for model.dbdemos_dbt_c360.dbt_c360_gold_churn_features (execute): 00:07:48.470225 => 00:07:53.572315
[0m00:07:53.575660 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: ROLLBACK
[0m00:07:53.577633 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:07:53.579013 [debug] [Thread-1 (]: On model.dbdemos_dbt_c360.dbt_c360_gold_churn_features: Close
[0m00:07:53.580365 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-75f7-15c0-8daf-d0c6369a9bc1
[0m00:07:53.653344 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87771f99-4138-4f3d-ac7e-ef94fe297ff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78ab9810>]}
[0m00:07:53.656516 [info ] [Thread-1 (]: 4 of 4 OK created sql table model dbdemos.dbt_c360_gold_churn_features ......... [[32mOK[0m in 5.21s]
[0m00:07:53.658251 [debug] [Thread-1 (]: Finished running node model.dbdemos_dbt_c360.dbt_c360_gold_churn_features
[0m00:07:53.660362 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:53.660985 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:07:53.858770 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01ee2691-790f-1fc3-b565-6fdd33135434
[0m00:07:53.859824 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:53.860705 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:53.861305 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:53.861916 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:53.862486 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:53.863050 [debug] [MainThread]: On master: Close
[0m00:07:53.863635 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01ee2691-790f-1fc3-b565-6fdd33135434
[0m00:07:53.932021 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:07:53.933441 [debug] [MainThread]: Connection 'model.dbdemos_dbt_c360.dbt_c360_gold_churn_features' was properly closed.
[0m00:07:53.936004 [info ] [MainThread]: 
[0m00:07:53.937250 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 23.27 seconds (23.27s).
[0m00:07:53.939190 [debug] [MainThread]: Command end result
[0m00:07:53.952717 [info ] [MainThread]: 
[0m00:07:53.953912 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:07:53.954804 [info ] [MainThread]: 
[0m00:07:53.955747 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m00:07:53.956997 [debug] [MainThread]: Command `dbt run` succeeded at 00:07:53.956826 after 23.94 seconds
[0m00:07:53.957791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8d593f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff78103940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff8dceee00>]}
[0m00:07:53.958592 [debug] [MainThread]: Flushing usage events
[0m00:07:57.079622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa63cbd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa4cc03d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa4cc00d0>]}


============================== 00:07:57.083011 | 4a55f5a5-72e2-4927-b7c5-a0538e547cac ==============================
[0m00:07:57.083011 [info ] [MainThread]: Running with dbt=1.5.3
[0m00:07:57.083787 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:07:57.491076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa89a7130>]}
[0m00:07:57.499877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff91bfec20>]}
[0m00:07:57.500511 [info ] [MainThread]: Registered adapter: databricks=1.5.5
[0m00:07:57.508853 [debug] [MainThread]: checksum: d91324b50612e0c4823cec6657ec795d80c11aba9d5b50e83fb26de63277dec4, vars: {}, profile: , target: , version: 1.5.3
[0m00:07:57.532383 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:07:57.532955 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:07:57.533581 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbdemos_dbt_c360.staging
[0m00:07:57.537841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff919fde10>]}
[0m00:07:57.545226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff919d0940>]}
[0m00:07:57.545744 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m00:07:57.546208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff919d0910>]}
[0m00:07:57.547300 [info ] [MainThread]: 
[0m00:07:57.547972 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:07:57.548900 [debug] [ThreadPool]: Acquiring new databricks connection 'list_asong_dev'
[0m00:07:57.549284 [debug] [ThreadPool]: Using databricks connection "list_asong_dev"
[0m00:07:57.550410 [debug] [ThreadPool]: On list_asong_dev: GetSchemas(database=`asong_dev`, schema=None)
[0m00:07:57.550762 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:07:57.729826 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7b5d-1740-b51a-f15fa5c43ff7
[0m00:07:58.452147 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m00:07:58.463365 [debug] [ThreadPool]: On list_asong_dev: Close
[0m00:07:58.465522 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-7b5d-1740-b51a-f15fa5c43ff7
[0m00:07:58.547309 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev, now create_asong_dev_dbdemos_dbt_test__audit)
[0m00:07:58.550812 [debug] [ThreadPool]: Creating schema "database: "asong_dev"
schema: "dbdemos_dbt_test__audit"
"
[0m00:07:58.575036 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:58.575852 [debug] [ThreadPool]: Using databricks connection "create_asong_dev_dbdemos_dbt_test__audit"
[0m00:07:58.576405 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "connection_name": "create_asong_dev_dbdemos_dbt_test__audit"} */
create schema if not exists `asong_dev`.`dbdemos_dbt_test__audit`
  
[0m00:07:58.577192 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:07:58.751444 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7bfa-1898-a146-bf12473cb887
[0m00:07:58.910299 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m00:07:58.912812 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:07:58.914118 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: ROLLBACK
[0m00:07:58.915104 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:07:58.915934 [debug] [ThreadPool]: On create_asong_dev_dbdemos_dbt_test__audit: Close
[0m00:07:58.916774 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-7bfa-1898-a146-bf12473cb887
[0m00:07:58.985739 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_asong_dev_dbdemos_dbt_test__audit, now list_asong_dev_dbdemos)
[0m00:07:58.991400 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos"
[0m00:07:58.992392 [debug] [ThreadPool]: On list_asong_dev_dbdemos: GetTables(database=asong_dev, schema=dbdemos, identifier=None)
[0m00:07:58.993091 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:07:59.167507 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7c39-155f-9750-383cdeec5eea
[0m00:07:59.985014 [debug] [ThreadPool]: SQL status: OK in 0.9900000095367432 seconds
[0m00:07:59.988184 [debug] [ThreadPool]: On list_asong_dev_dbdemos: Close
[0m00:07:59.988762 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-7c39-155f-9750-383cdeec5eea
[0m00:08:00.048626 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos, now list_asong_dev_dbdemos_dbt_test__audit)
[0m00:08:00.052002 [debug] [ThreadPool]: Using databricks connection "list_asong_dev_dbdemos_dbt_test__audit"
[0m00:08:00.052641 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: GetTables(database=asong_dev, schema=dbdemos_dbt_test__audit, identifier=None)
[0m00:08:00.053184 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:08:00.239083 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7cdc-1646-bc2c-ce07705ca0a6
[0m00:08:00.908018 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m00:08:00.911090 [debug] [ThreadPool]: On list_asong_dev_dbdemos_dbt_test__audit: Close
[0m00:08:00.911686 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01ee2691-7cdc-1646-bc2c-ce07705ca0a6
[0m00:08:00.974612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a55f5a5-72e2-4927-b7c5-a0538e547cac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff91b8eb90>]}
[0m00:08:00.975648 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:00.976294 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:08:00.977235 [info ] [MainThread]: Concurrency: 1 threads (target='local')
[0m00:08:00.978087 [info ] [MainThread]: 
[0m00:08:00.991772 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m00:08:00.992864 [info ] [Thread-1 (]: 1 of 6 START test accepted_values_dbt_c360_silver_users_churn__1__0 ............ [RUN]
[0m00:08:00.993781 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_asong_dev_dbdemos_dbt_test__audit, now test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3)
[0m00:08:00.994440 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m00:08:01.003125 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m00:08:01.005201 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3 (compile): 00:08:00.995056 => 00:08:01.005006
[0m00:08:01.007060 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m00:08:01.016951 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m00:08:01.019439 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:01.019902 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"
[0m00:08:01.020462 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        churn as value_field,
        count(*) as n_records

    from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
    group by churn

)

select *
from all_values
where value_field not in (
    '1','0'
)



      
    ) dbt_internal_test
[0m00:08:01.020889 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:01.185615 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7d6d-18be-ae5a-9480c7e26815
[0m00:08:01.586982 [debug] [Thread-1 (]: SQL status: OK in 0.5699999928474426 seconds
[0m00:08:01.592204 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3 (execute): 00:08:01.007464 => 00:08:01.591948
[0m00:08:01.593426 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: ROLLBACK
[0m00:08:01.594212 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:01.594831 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3: Close
[0m00:08:01.595489 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-7d6d-18be-ae5a-9480c7e26815
[0m00:08:01.665315 [info ] [Thread-1 (]: 1 of 6 PASS accepted_values_dbt_c360_silver_users_churn__1__0 .................. [[32mPASS[0m in 0.67s]
[0m00:08:01.666792 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3
[0m00:08:01.667994 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m00:08:01.668774 [info ] [Thread-1 (]: 2 of 6 START test assert_orders_amount_must_be_above_zero ...................... [RUN]
[0m00:08:01.669827 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.accepted_values_dbt_c360_silver_users_churn__1__0.23dddbabf3, now test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero)
[0m00:08:01.670958 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m00:08:01.676223 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m00:08:01.679107 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero (compile): 00:08:01.671720 => 00:08:01.678741
[0m00:08:01.679994 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m00:08:01.690048 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m00:08:01.691138 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"} */
drop table if exists `asong_dev`.`dbdemos_dbt_test__audit`.`assert_orders_amount_must_be_above_zero`
[0m00:08:01.692025 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:01.870237 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-7dd6-13ad-a624-0df4b549550f
[0m00:08:02.664199 [debug] [Thread-1 (]: SQL status: OK in 0.9700000286102295 seconds
[0m00:08:02.727275 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:02.728363 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m00:08:02.729172 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"} */

        
  
    
        create or replace table `asong_dev`.`dbdemos_dbt_test__audit`.`assert_orders_amount_must_be_above_zero`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- notes: quarantine records and isolate them if the total sales amount is negative -- 
select 
 user_id,
 sum(amount) as total_amount 
from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
group by 1 
having not (total_amount >= 0)
  
    
[0m00:08:05.270422 [debug] [Thread-1 (]: SQL status: OK in 2.5399999618530273 seconds
[0m00:08:05.279449 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:08:05.284844 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m00:08:05.293079 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"
[0m00:08:05.294382 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `asong_dev`.`dbdemos_dbt_test__audit`.`assert_orders_amount_must_be_above_zero`
    
      
    ) dbt_internal_test
[0m00:08:05.629186 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:08:05.642270 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero (execute): 00:08:01.680641 => 00:08:05.641704
[0m00:08:05.643779 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: ROLLBACK
[0m00:08:05.645029 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:05.646444 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero: Close
[0m00:08:05.647705 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-7dd6-13ad-a624-0df4b549550f
[0m00:08:05.716353 [info ] [Thread-1 (]: 2 of 6 PASS assert_orders_amount_must_be_above_zero ............................ [[32mPASS[0m in 4.05s]
[0m00:08:05.719079 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero
[0m00:08:05.722214 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m00:08:05.725341 [info ] [Thread-1 (]: 3 of 6 START test not_null_dbt_c360_silver_users_user_id ....................... [RUN]
[0m00:08:05.729723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.assert_orders_amount_must_be_above_zero, now test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5)
[0m00:08:05.731214 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m00:08:05.742271 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m00:08:05.745793 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5 (compile): 00:08:05.731913 => 00:08:05.745361
[0m00:08:05.746936 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m00:08:05.751261 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m00:08:05.754110 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:05.754919 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"
[0m00:08:05.755669 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
where user_id is null



      
    ) dbt_internal_test
[0m00:08:05.756853 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:05.959582 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-8045-17d2-ad97-fa7dd8a17b64
[0m00:08:06.358369 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m00:08:06.364320 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5 (execute): 00:08:05.747735 => 00:08:06.363940
[0m00:08:06.365788 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: ROLLBACK
[0m00:08:06.366992 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:06.368018 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5: Close
[0m00:08:06.369050 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-8045-17d2-ad97-fa7dd8a17b64
[0m00:08:06.431768 [info ] [Thread-1 (]: 3 of 6 PASS not_null_dbt_c360_silver_users_user_id ............................. [[32mPASS[0m in 0.70s]
[0m00:08:06.433500 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5
[0m00:08:06.434529 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m00:08:06.436281 [info ] [Thread-1 (]: 4 of 6 START test relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_  [RUN]
[0m00:08:06.437593 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.not_null_dbt_c360_silver_users_user_id.b64ba654d5, now test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838)
[0m00:08:06.438422 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m00:08:06.449094 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m00:08:06.451208 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838 (compile): 00:08:06.439061 => 00:08:06.451009
[0m00:08:06.451805 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m00:08:06.454285 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m00:08:06.455798 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:06.456284 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"
[0m00:08:06.457149 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select user_id as from_field
    from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m00:08:06.457735 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:06.640743 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-80ad-1a0e-b86a-fa735332c0a9
[0m00:08:07.268765 [debug] [Thread-1 (]: SQL status: OK in 0.8100000023841858 seconds
[0m00:08:07.276574 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838 (execute): 00:08:06.452261 => 00:08:07.276077
[0m00:08:07.278459 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: ROLLBACK
[0m00:08:07.279717 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:07.281040 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838: Close
[0m00:08:07.282460 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-80ad-1a0e-b86a-fa735332c0a9
[0m00:08:07.348242 [info ] [Thread-1 (]: 4 of 6 PASS relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_  [[32mPASS[0m in 0.91s]
[0m00:08:07.350812 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838
[0m00:08:07.354799 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m00:08:07.356648 [info ] [Thread-1 (]: 5 of 6 START test unique_dbt_c360_silver_orders_order_id ....................... [RUN]
[0m00:08:07.358738 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.relationships_dbt_c360_silver_orders_user_id__user_id__ref_dbt_c360_silver_users_.ad4f2b2838, now test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9)
[0m00:08:07.360131 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m00:08:07.377150 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m00:08:07.380422 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9 (compile): 00:08:07.361303 => 00:08:07.380160
[0m00:08:07.381261 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m00:08:07.387635 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m00:08:07.390803 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:07.391587 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"
[0m00:08:07.392445 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `asong_dev`.`dbdemos`.`dbt_c360_silver_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:08:07.393577 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:07.583148 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-813d-14dc-a6bc-887d2fbe9188
[0m00:08:08.255862 [debug] [Thread-1 (]: SQL status: OK in 0.8600000143051147 seconds
[0m00:08:08.258898 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9 (execute): 00:08:07.381979 => 00:08:08.258705
[0m00:08:08.260453 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: ROLLBACK
[0m00:08:08.261060 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:08.261588 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9: Close
[0m00:08:08.262096 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-813d-14dc-a6bc-887d2fbe9188
[0m00:08:08.330321 [info ] [Thread-1 (]: 5 of 6 PASS unique_dbt_c360_silver_orders_order_id ............................. [[32mPASS[0m in 0.97s]
[0m00:08:08.331565 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9
[0m00:08:08.332440 [debug] [Thread-1 (]: Began running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m00:08:08.333262 [info ] [Thread-1 (]: 6 of 6 START test unique_dbt_c360_silver_users_user_id ......................... [RUN]
[0m00:08:08.334347 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbdemos_dbt_c360.unique_dbt_c360_silver_orders_order_id.99c8c011b9, now test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63)
[0m00:08:08.335120 [debug] [Thread-1 (]: Began compiling node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m00:08:08.340662 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m00:08:08.343043 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63 (compile): 00:08:08.335718 => 00:08:08.342739
[0m00:08:08.343703 [debug] [Thread-1 (]: Began executing node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m00:08:08.346473 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m00:08:08.348319 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:08.348966 [debug] [Thread-1 (]: Using databricks connection "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"
[0m00:08:08.349530 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: /* {"app": "dbt", "dbt_version": "1.5.3", "dbt_databricks_version": "1.5.5", "databricks_sql_connector_version": "2.7.0", "profile_name": "dbdemos_dbt_c360", "target_name": "local", "node_id": "test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `asong_dev`.`dbdemos`.`dbt_c360_silver_users`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:08:08.350267 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:08:08.531380 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01ee2691-81cd-1c4e-b20e-ac0c47c8fbf3
[0m00:08:09.059608 [debug] [Thread-1 (]: SQL status: OK in 0.7099999785423279 seconds
[0m00:08:09.065337 [debug] [Thread-1 (]: Timing info for test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63 (execute): 00:08:08.344243 => 00:08:09.064930
[0m00:08:09.066617 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: ROLLBACK
[0m00:08:09.067613 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:08:09.068525 [debug] [Thread-1 (]: On test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63: Close
[0m00:08:09.069499 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01ee2691-81cd-1c4e-b20e-ac0c47c8fbf3
[0m00:08:09.132189 [info ] [Thread-1 (]: 6 of 6 PASS unique_dbt_c360_silver_users_user_id ............................... [[32mPASS[0m in 0.80s]
[0m00:08:09.133364 [debug] [Thread-1 (]: Finished running node test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63
[0m00:08:09.135075 [debug] [MainThread]: On master: ROLLBACK
[0m00:08:09.135664 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:08:09.303009 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01ee2691-8244-17b0-bb66-689ff4a4b3a9
[0m00:08:09.305138 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:09.306297 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:09.307202 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:08:09.308098 [debug] [MainThread]: On master: ROLLBACK
[0m00:08:09.308986 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:09.309799 [debug] [MainThread]: On master: Close
[0m00:08:09.310860 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01ee2691-8244-17b0-bb66-689ff4a4b3a9
[0m00:08:09.387767 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:08:09.393011 [debug] [MainThread]: Connection 'test.dbdemos_dbt_c360.unique_dbt_c360_silver_users_user_id.9f3c723e63' was properly closed.
[0m00:08:09.397938 [info ] [MainThread]: 
[0m00:08:09.402722 [info ] [MainThread]: Finished running 6 tests in 0 hours 0 minutes and 11.85 seconds (11.85s).
[0m00:08:09.405717 [debug] [MainThread]: Command end result
[0m00:08:09.422101 [info ] [MainThread]: 
[0m00:08:09.423604 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:08:09.424470 [info ] [MainThread]: 
[0m00:08:09.425310 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m00:08:09.426423 [debug] [MainThread]: Command `dbt test` succeeded at 00:08:09.426295 after 12.35 seconds
[0m00:08:09.427198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffffa63cbd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff91bfec20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff91b8eb90>]}
[0m00:08:09.427986 [debug] [MainThread]: Flushing usage events
